{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "oneshot learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9EUfmmBja-_"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import skimage.io as io\n",
        "import pickle\n",
        "\n",
        "import time\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
        "from keras.models import Model\n",
        "\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.layers.core import Lambda, Flatten, Dense\n",
        "from keras.initializers import glorot_uniform\n",
        "\n",
        "from tensorflow.keras.layers import Layer\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "# from tensorflow.keras.preprocessing import image\n",
        "# from tensorflow.keras.models import Model\n",
        "# from tensorflow.keras.layers import Dense, GlobalAveragePooling2D"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXwbeZYYjbAJ"
      },
      "source": [
        "## Loading data\n",
        "a function that loads the images locations and labels.<br>\n",
        "input: the data path.<br>\n",
        "output:<br>\n",
        "&emsp;&emsp;dataHiero -> a dataframe with index= location of images and label= their labels <br>\n",
        "&emsp;&emsp;img_groups -> a dictionary in the shape of { \"label\" : [array of locations of images labeled with this label] }"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noWbwGlLjbAY"
      },
      "source": [
        "path=\"../GlyphDataset/Dataset/Manual/Preprocessed/\"\n",
        "\n",
        "def loadData(folderPictures=path):\n",
        "    \n",
        "    folders=next(os.walk(folderPictures))[1]\n",
        "    img_groups = {}\n",
        "    img_list={}\n",
        "\n",
        "    for folder in folders:\n",
        "        for img_file in os.listdir(folderPictures+folder):\n",
        "            name, \n",
        "            label = img_file.strip('.png').split(\"_\")\n",
        "            \n",
        "            \n",
        "            # One image per class\n",
        "\n",
        "            #if label not in img_groups.keys():\n",
        "            #    img_groups[label] = [folder + \"_\" + name]\n",
        "\n",
        "\n",
        "            # Multiple images per class\n",
        "\n",
        "            if label in img_groups.keys():\n",
        "                img_groups[label].append(folder+\"_\"+name)\n",
        "            else:\n",
        "                img_groups[label] = [folder+\"_\"+name]\n",
        "\n",
        "            img_list[folder+\"_\"+name]=[label]\n",
        "\n",
        "\n",
        "    # Remove class with only one hieroglyph\n",
        "\n",
        "\n",
        "    for k,v in list(img_groups.items()):\n",
        "        if len(v)==1: del img_groups[k]\n",
        "\n",
        "    # Extract only N hieroglyph classes randomly\n",
        "\n",
        "    nclass = len(img_groups.keys())\n",
        "\n",
        "    list_of_class = random.sample(list(img_groups.keys()), nclass)\n",
        "#     print(list_of_class)\n",
        "\n",
        "    short_dico = {x: img_groups[x] for x in list_of_class if x in img_groups}\n",
        "\n",
        "    dataHiero=pd.DataFrame.from_dict(img_list,orient='index')\n",
        "    dataHiero.columns = [\"label\"]\n",
        "    dataHiero = dataHiero[dataHiero.label != 'UNKNOWN']\n",
        "\n",
        "    dataHiero = dataHiero.loc[dataHiero['label'].isin(short_dico)]\n",
        "\n",
        "\n",
        "    dataHiero.reset_index(level=0, inplace=True)\n",
        "\n",
        "    return dataHiero,img_groups"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CaZM8DxjbAp"
      },
      "source": [
        "a function that takes the image groups and load those images<br>\n",
        "input: img_proups dictionary<br>\n",
        "output:<br>\n",
        "&emsp;&emsp;X -> np array of the images<br>\n",
        "&emsp;&emsp;y -> np array of labels<br>\n",
        "&emsp;&emsp;glyph_sizes -> a dictionary in the form of {'label' : (starting index, ending index in X and y)}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO2UaDgTjbAu"
      },
      "source": [
        "def read_images(img_groups,path):\n",
        "    X=[]\n",
        "    y=[]\n",
        "    glyph_sizes={}\n",
        "    low=0\n",
        "    for glyph in img_groups:\n",
        "        category_images=[]\n",
        "        high=low\n",
        "        for img_path in img_groups[glyph] :\n",
        "            folder,name = img_path.split('_')\n",
        "            image = io.imread(path+folder+'/'+name+'_'+glyph+'.png')\n",
        "            X.append(image)\n",
        "            y.append(glyph)\n",
        "            high+=1\n",
        "#         X.append(np.array(category_images))\n",
        "        glyph_sizes[glyph]=(low,high-1)\n",
        "        low=high\n",
        "        \n",
        "    return np.array(X),np.array(y).reshape((-1,1)),glyph_sizes\n",
        "            \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYwf13e4jbA3",
        "outputId": "8e63b4ed-ab67-4a8b-f421-4fc94850f81e"
      },
      "source": [
        "dataHiero,img_groups=loadData(folderPictures=path)\n",
        "dataHiero.head()\n",
        "# img_groups"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>39_390115</td>\n",
              "      <td>D21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>39_390082</td>\n",
              "      <td>M17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>39_390292</td>\n",
              "      <td>V31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>39_390375</td>\n",
              "      <td>U15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>39_390175</td>\n",
              "      <td>D35</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       index label\n",
              "0  39_390115   D21\n",
              "1  39_390082   M17\n",
              "2  39_390292   V31\n",
              "3  39_390375   U15\n",
              "4  39_390175   D35"
            ]
          },
          "execution_count": 399,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-j6RWKkrjbA6"
      },
      "source": [
        "X,y,sizes=read_images(img_groups,path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "kNTAe8oQjbBG",
        "outputId": "a030e446-b195-4481-b03e-2b201b17045f"
      },
      "source": [
        "type(X)\n",
        "print(y.shape)\n",
        "print(X.shape)\n",
        "sizes['D21'][1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2921, 1)\n",
            "(2921, 75, 50)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjQdogROjbBI"
      },
      "source": [
        "saving the images into a pickle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br2-Xm7bjbBb"
      },
      "source": [
        "#train val split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,stratify=y, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPS8xCJPjbBh"
      },
      "source": [
        "def get_sizes(X,Y):\n",
        "    sizes={}\n",
        "    for i ,(x,y) in enumerate(zip(X,Y)):\n",
        "#         print(i,x,y)\n",
        "        if y[0] in sizes:\n",
        "            sizes[y[0]].append(i)\n",
        "        else:\n",
        "            sizes[y[0]]=[i]\n",
        "    return sizes\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sG1c9UUHjbBi"
      },
      "source": [
        "sizes=get_sizes(X_train,y_train)\n",
        "X=X_train\n",
        "y=y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbrU-fAkjbBn"
      },
      "source": [
        "#saving data as pickle\n",
        "with open(\"train.pickle\", \"wb\") as f:\n",
        "    pickle.dump((X,y,sizes),f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4szlW2XjbBo"
      },
      "source": [
        "sizes_val=get_sizes(X_test,y_test)\n",
        "Xval=X_test\n",
        "yval=y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkrX6IOujbBp"
      },
      "source": [
        "#saving data as pickle\n",
        "with open(\"test.pickle\", \"wb\") as f:\n",
        "    pickle.dump((Xval,yval,sizes_val),f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUfovN9vjbBw"
      },
      "source": [
        "# #saving data as pickle\n",
        "# with open(\"test.pickle\", \"wb\") as f:\n",
        "#     pickle.dump((Xval,yval,sizes_val),f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtd7kUY6jbBx"
      },
      "source": [
        "## reading the training tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVz6vsHZ4SeJ"
      },
      "source": [
        "#colab\n",
        "data_path= '/content/drive/MyDrive/hiero_cv/'\n",
        "#local\n",
        "# data_path='./'"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZPC6wKrjbB4"
      },
      "source": [
        "with open(data_path+\"train.pickle\", \"rb\") as f:\n",
        "    (X,y,sizes) = pickle.load(f)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOstoN_vjbB7"
      },
      "source": [
        "with open(data_path+\"test.pickle\", \"rb\") as f:\n",
        "    (Xval,yval,sizes_val) = pickle.load(f)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmdcIQiwpDLV",
        "outputId": "0d44f9a1-1115-4f46-df94-5903a8bde210"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOB0W_VqjbB8"
      },
      "source": [
        "def initialize_weights(shape, name=None):\n",
        "    \"\"\"\n",
        "        The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
        "        suggests to initialize CNN layer weights with mean as 0.0 and standard deviation of 0.01\n",
        "    \"\"\"\n",
        "    return np.random.normal(loc = 0.0, scale = 1e-2, size = shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xwz8vTAmjbCB"
      },
      "source": [
        "def initialize_bias(shape, name=None):\n",
        "    \"\"\"\n",
        "        The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
        "        suggests to initialize CNN layer bias with mean as 0.5 and standard deviation of 0.01\n",
        "    \"\"\"\n",
        "    return np.random.normal(loc = 0.5, scale = 1e-2, size = shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xJN3eoVjbCQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G63fmh8-jbCR"
      },
      "source": [
        "# def get_siamese_model_2(input_shape):\n",
        "#     \"\"\"\n",
        "#         Model architecture based on the one provided in: http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
        "#     \"\"\"\n",
        "    \n",
        "#     # Define the tensors for the two input images\n",
        "#     left_input = Input(input_shape)\n",
        "#     right_input = Input(input_shape)\n",
        "    \n",
        "#     # Convolutional Neural Network\n",
        "#     model = Sequential()\n",
        "#     model.add(Conv2D(64, (3,3),strides=(2, 2), activation='relu', input_shape=input_shape, kernel_regularizer=l2(2e-4)))\n",
        "#     model.add(MaxPooling2D((2, 2), strides=2))\n",
        "#     model.add(Conv2D(64, (3,3), activation='relu', kernel_regularizer=l2(2e-4)))\n",
        "#     model.add(MaxPooling2D((2, 2), strides=2))\n",
        "#     model.add(Conv2D(128, (3,3), activation='relu', kernel_regularizer=l2(2e-4)))\n",
        "# #     model.add(MaxPooling2D())\n",
        "# #     model.add(Conv2D(256, (4,4), activation='relu',  kernel_regularizer=l2(2e-4)))\n",
        "#     model.add(Flatten())\n",
        "#     model.add(Dense(4096, activation='sigmoid',\n",
        "#                    kernel_regularizer=l2(1e-3)))\n",
        "    \n",
        "#     # Generate the encodings (feature vectors) for the two images\n",
        "#     encoded_l = model(left_input)\n",
        "#     encoded_r = model(right_input)\n",
        "    \n",
        "#     # Add a customized layer to compute the absolute difference between the encodings\n",
        "#     L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
        "#     L1_distance = L1_layer([encoded_l, encoded_r])\n",
        "    \n",
        "#     # Add a dense layer with a sigmoid unit to generate the similarity score\n",
        "#     prediction = Dense(1,activation='sigmoid')(L1_distance)\n",
        "    \n",
        "#     # Connect the inputs with the outputs\n",
        "#     siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
        "    \n",
        "#     # return the model\n",
        "#     return siamese_net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "pLzzLePsjbCS"
      },
      "source": [
        "# model_2 = get_siamese_model_2((75, 50, 1))\n",
        "# model_2.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk2Tsft9jbCT"
      },
      "source": [
        "# optimizer = Adam(lr = 0.001)\n",
        "# model_2.compile(loss=\"binary_crossentropy\",optimizer=optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFcNXLnxjbCT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zodoNhCYjbCU"
      },
      "source": [
        "# def transfer_model(input_shape):\n",
        "#     \"\"\"\n",
        "#         Model architecture based on the one provided in: http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
        "#     \"\"\"\n",
        "    \n",
        "#     # Define the tensors for the two input images\n",
        "#     left_input = Input(input_shape)\n",
        "#     right_input = Input(input_shape)\n",
        "    \n",
        "#     #Import inception model for transfer learning without output layers\n",
        "#     base_model = InceptionV3(weights='imagenet', include_top=False, input_shape = input_shape)\n",
        "    \n",
        "    \n",
        "#     x = base_model.output\n",
        "#     x = GlobalAveragePooling2D()(x)\n",
        "#     # let's add a fully-connected layer\n",
        "#     x = Dense(1024, activation='relu')(x)\n",
        "#     # and a logistic layer -- let's say we have 200 classes\n",
        "#     model= Dense(200, activation='softmax')(x)\n",
        "    \n",
        "#     # this is the model we will train\n",
        "# #     model = Model(inputs=base_model.input, outputs=predictions)\n",
        "        \n",
        "    \n",
        "#     # Generate the encodings (feature vectors) for the two images\n",
        "#     encoded_l = model(left_input)\n",
        "#     encoded_r = model(right_input)\n",
        "    \n",
        "#     # Add a customized layer to compute the absolute difference between the encodings\n",
        "#     L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
        "#     L1_distance = L1_layer([encoded_l, encoded_r])\n",
        "    \n",
        "#     # Add a dense layer with a sigmoid unit to generate the similarity score\n",
        "#     prediction = Dense(1,activation='sigmoid')(L1_distance)\n",
        "    \n",
        "#     # Connect the inputs with the outputs\n",
        "#     model = Model(inputs=[left_input,right_input],outputs=prediction)\n",
        "    \n",
        "#     for layer in model.layers[:249]:\n",
        "#        layer.trainable = False\n",
        "#     for layer in model.layers[249:]:\n",
        "#        layer.trainable = True\n",
        "    \n",
        "#     # return the model\n",
        "#     return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-fIRWtajbCV"
      },
      "source": [
        "# inception_model = transfer_model((75, 50, 1))\n",
        "# inception_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEfmyM_KjbCW"
      },
      "source": [
        "# inception_model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpR7qVOCjbCX"
      },
      "source": [
        "def get_siamese_model(input_shape):\n",
        "    \"\"\"\n",
        "        Model architecture based on the one provided in: http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define the tensors for the two input images\n",
        "    left_input = Input(input_shape)\n",
        "    right_input = Input(input_shape)\n",
        "    \n",
        "    # Convolutional Neural Network\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(64, (3,3), activation='relu', input_shape=input_shape, kernel_regularizer=l2(2e-4)))\n",
        "    model.add(MaxPooling2D())\n",
        "    model.add(Conv2D(64, (3,3), activation='relu', kernel_regularizer=l2(2e-4)))\n",
        "    model.add(MaxPooling2D())\n",
        "    model.add(Conv2D(128, (4,4), activation='relu', kernel_regularizer=l2(2e-4)))\n",
        "    model.add(MaxPooling2D())\n",
        "    model.add(Conv2D(256, (4,4), activation='relu',  kernel_regularizer=l2(2e-4)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(4096, activation='sigmoid',\n",
        "                   kernel_regularizer=l2(1e-3)))\n",
        "    \n",
        "    # Generate the encodings (feature vectors) for the two images\n",
        "    encoded_l = model(left_input)\n",
        "    encoded_r = model(right_input)\n",
        "    \n",
        "    # Add a customized layer to compute the absolute difference between the encodings\n",
        "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
        "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
        "    \n",
        "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
        "    prediction = Dense(1,activation='sigmoid')(L1_distance)\n",
        "    \n",
        "    # Connect the inputs with the outputs\n",
        "    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
        "    \n",
        "    # return the model\n",
        "    return siamese_net"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MtvWYCyjbCb",
        "outputId": "9f7a6788-9050-4efb-a4eb-58b8b8ccf745"
      },
      "source": [
        "model = get_siamese_model((75, 50, 1))\n",
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 75, 50, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            [(None, 75, 50, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential_1 (Sequential)       (None, 4096)         4891712     input_3[0][0]                    \n",
            "                                                                 input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 4096)         0           sequential_1[0][0]               \n",
            "                                                                 sequential_1[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1)            4097        lambda_1[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 4,895,809\n",
            "Trainable params: 4,895,809\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpDd_9CWjbCd",
        "outputId": "1c208ebe-ce82-48db-bdfd-adb398815959"
      },
      "source": [
        "optimizer = Adam(lr = 0.0001)\n",
        "model.compile(loss=\"binary_crossentropy\",optimizer=optimizer)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNSpnQaljbCe"
      },
      "source": [
        "a function that create pairs of images with y= 1 if they are similar and 0 if they are different. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUBHJas8jbCf"
      },
      "source": [
        "## a function to predict which glyph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kua3lTs3jbCf"
      },
      "source": [
        "def create_glyphlist(X,sizes):\n",
        "    images=[]\n",
        "    labels=[]\n",
        "    _,w,h=X.shape\n",
        "    for glyph in sizes:\n",
        "        index=sizes[glyph][0]\n",
        "        images.append(X[index].reshape( w , h, 1))\n",
        "        labels.append(glyph)\n",
        "    return np.asarray(images), np.asarray(labels) "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTWwXWyBjbCn"
      },
      "source": [
        "anchor_img, anchor_label=create_glyphlist(X,sizes)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UV5F8TRwjbCo"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cj5krLHkjbCo"
      },
      "source": [
        "def whichGlyph_pair(image,anchor_img,anchor_label):\n",
        "    N,w,h,_=anchor_img.shape\n",
        "#     pairs=[np.zeros((N, w, h,1)) for i in range(2)]\n",
        "    \n",
        "    test_image= np.asarray([image]*N).reshape(N, w, h,1)\n",
        "    \n",
        "    anchor_label, test_image, anchor_img = shuffle(anchor_label, test_image, anchor_img)\n",
        "#     pairs = [test_image,anchor_img]\n",
        "    \n",
        "    return test_image, anchor_img, anchor_label\n",
        "    "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzVYcJ4tjbCp"
      },
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrc8caDFjbCp"
      },
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6a5mx_MjbCq"
      },
      "source": [
        "def whichGlyph(model,image,anchor_img,anchor_label):\n",
        "    test_image,anchor_img,targets = whichGlyph_pair(image,anchor_img,anchor_label)\n",
        "    probs = model.predict([test_image,anchor_img])\n",
        "    return probs,anchor_img,targets\n",
        "    "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUxUIQb8jbC1"
      },
      "source": [
        "# def whichGlyph(model,image,anchor_img,anchor_label):\n",
        "#     test_image,anchor_img,targets = whichGlyph_pair(image,anchor_img,anchor_label)\n",
        "#     probs=[]\n",
        "#     for i in range(0,len(targets),2):\n",
        "#         pair=[[test_image[i],test_image[i+1]],[anchor_img[i],anchor_img[i+1]]]\n",
        "#         pred=model.predict(pair)\n",
        "#         for p in pred:\n",
        "#             probs.append(p)\n",
        "        \n",
        "#     return probs,anchor_img,targets"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kb2Uo1PrjbC2"
      },
      "source": [
        ""
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arWKCACTjbC4"
      },
      "source": [
        ""
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2SsLM6ujbC5"
      },
      "source": [
        ""
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eM5fQribjbC5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xCa5bCWjbC6"
      },
      "source": [
        "## creating pairs of images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7t5J2GCjbC7"
      },
      "source": [
        "def createPairs(X,y,sizes,batch_size):\n",
        "    ##create a batch with half it's size are similar glyphs and the other half are different.\n",
        "    n=0\n",
        "    i=0\n",
        "    \n",
        "    label=[]\n",
        "    _,w,h=X.shape\n",
        "    # initialize 2 empty arrays for the input image batch\n",
        "#     pairs=[np.zeros((batch_size, w, h,1)) for i in range(2)]\n",
        "    input1=np.zeros((batch_size, w, h,1))\n",
        "    input2=np.zeros((batch_size, w, h,1))\n",
        "    \n",
        "    while n < batch_size:\n",
        "        random_key1=random.choice(list(sizes))\n",
        "#         low=sizes[random_key1][0]\n",
        "#         high=sizes[random_key1][1]\n",
        "        index1, index3 = np.random.choice(sizes[random_key1], size=2)\n",
        "        index2 = np.random.choice(sizes[random_key1])\n",
        "        random_key2=random.choice(list(sizes))\n",
        "        \n",
        "        while random_key2 == random_key1:\n",
        "            random_key2=random.choice(list(sizes))\n",
        "            \n",
        "#         low=sizes[random_key2][0]\n",
        "#         high=sizes[random_key2][1]\n",
        "        index4=np.random.choice(sizes[random_key2])\n",
        "        n += 2\n",
        "        # appending images 1 and 3 into input1 and input2 corresponding to y=1 \n",
        "        #and images 2 and 4 corresponding to y=0\n",
        "    \n",
        "        input1[i,:,:,:] = X[index1].reshape( w , h, 1)\n",
        "        input1[i+1,:,:,:] = X[index2].reshape(w, h, 1)\n",
        "        input2[i,:,:,:] = X[index3].reshape(w, h, 1)\n",
        "        input2[i+1,:,:,:] = X[index4].reshape(w, h, 1)\n",
        "        i += 2\n",
        "#         input1+=[X[index1],X[index2]]\n",
        "#         input2+=[X[index3],X[index4]]\n",
        "        label+=[1,0]\n",
        "        \n",
        "#         print(index1,index2,index3,index4)\n",
        "#         print(y[index1],y[index2],y[index3],y[index4])\n",
        "#         print(random_key1,random_key2)\n",
        "    input1,input2,label = shuffle(input1,input2,label)\n",
        "    pairs=[input1,input2]\n",
        "    \n",
        "    return pairs,label\n",
        "pairs,label=createPairs(X,y,sizes,32)   "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXoehBUTjbC9"
      },
      "source": [
        "# pairs[1].shape\n",
        "# label"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alqGHaxLjbC-"
      },
      "source": [
        "a fn that creates a N-way one shot learning task where it create pairs with the wanted image and N-1 different ones and 1 similar one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVWV2CnbjbC-"
      },
      "source": [
        "def make_oneshot_task(X,y,sizes,N):\n",
        "    _,w,h=X.shape\n",
        "    pairs=[np.zeros((N, w, h,1)) for i in range(2)]\n",
        "    \n",
        "    true_key=random.choice(list(sizes))\n",
        "#     low=sizes[true_key][0]\n",
        "#     high=sizes[true_key][1]\n",
        "    \n",
        "    index=np.random.choice(sizes[true_key])\n",
        "    index2=np.random.choice(sizes[true_key])\n",
        "    \n",
        "    test_image= np.asarray([X[index]]*N).reshape(N, w, h,1)\n",
        "#     print(X.shape)\n",
        "    X_diff= np.delete(X,sizes[true_key],axis=0)\n",
        "#     print(X_diff.shape)\n",
        "    indices= np.random.choice(range(0, len(X_diff)), size = N-1)\n",
        "    \n",
        "    support_set=X_diff[indices,:,:]\n",
        "    ##!!! adding the similar image to the start of the array is not working!!!!!!!!!!!!!!!!!!!!!\n",
        "    \n",
        "#     print(support_set.shape)\n",
        "#     support_set = [ X[index2] ] + support_set\n",
        "    support_set=np.insert(support_set,0,X[index2],axis=0)\n",
        "#     print(support_set.shape)\n",
        "    targets = np.zeros((N,))\n",
        "    targets[0] = 1\n",
        "    support_set=support_set.reshape(N,w,h,1)\n",
        "    targets, test_image, support_set = shuffle(targets, test_image, support_set)\n",
        "    pairs = [test_image,support_set]\n",
        "\n",
        "    return pairs, targets\n",
        "    \n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27BD6Rg9jbDO",
        "outputId": "89a6366c-8e47-48a1-f764-25e90d53e787"
      },
      "source": [
        "pp,tt=make_oneshot_task(Xval,yval,sizes_val,20) \n",
        "pp[0].shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 75, 50, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6lenk9bjbDP",
        "outputId": "976c09a3-38bc-4793-b322-15e73a004764"
      },
      "source": [
        "# plt.imshow(pairs[0][0])\n",
        "pairs[1].shape\n",
        "# len(label)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32, 75, 50, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr6bSOuBjbDZ"
      },
      "source": [
        ""
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8POHa_JjbDZ"
      },
      "source": [
        "def test_oneshot(model,X,y,sizes, N, k, s = \"val\", verbose = 0):\n",
        "    \"\"\"Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks\"\"\"\n",
        "    n_correct = 0\n",
        "    if verbose:\n",
        "        print(\"Evaluating model on {} random {} way one-shot learning tasks ... \\n\".format(k,N))\n",
        "    for i in range(k):\n",
        "        inputs, targets = make_oneshot_task(X,y,sizes,N)\n",
        "        probs = model.predict(inputs)\n",
        "        if np.argmax(probs) == np.argmax(targets):\n",
        "            n_correct+=1\n",
        "    percent_correct = (100.0 * n_correct / k)\n",
        "    if verbose:\n",
        "        print(\"Got an average of {}% {} way one-shot learning accuracy \\n\".format(percent_correct,N))\n",
        "    return percent_correct"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbUqy4FNjbDf"
      },
      "source": [
        "# Hyper parameters\n",
        "evaluate_every = 200 # interval for evaluating on one-shot tasks\n",
        "batch_size = 32\n",
        "n_iter = 20000 # No. of training iterations 20000\n",
        "N_way = 20 # how many classes for testing one-shot tasks\n",
        "n_val = 250 # how many one-shot tasks to validate on\n",
        "best = -1"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJXH6WomjbDg"
      },
      "source": [
        "model_path = './weights/'\n",
        "model_2_path= '/content/drive/MyDrive/hiero_cv/weights/'"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUJcfqJLjbDh",
        "outputId": "35d706cb-5ec6-4b11-caa4-de8edd5e69c5"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2921, 75, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwLw68YljbDj",
        "outputId": "2b663216-cd1c-408e-f1ab-b1a758f23866"
      },
      "source": [
        "print(\"Starting training process!\")\n",
        "print(\"-------------------------------------\")\n",
        "t_start = time.time()\n",
        "for i in range(1, n_iter+1):\n",
        "    (inputs,targets) = createPairs(X,y,sizes,batch_size)\n",
        "    targets=np.asarray(targets)\n",
        "    loss = model.train_on_batch(inputs, targets)\n",
        "    if i % evaluate_every == 0:\n",
        "        print(\"\\n ------------- \\n\")\n",
        "        print(\"Time for {0} iterations: {1} mins\".format(i, (time.time()-t_start)/60.0))\n",
        "        print(\"Train Loss: {0}\".format(loss)) \n",
        "        val_acc = test_oneshot(model,Xval,yval,sizes_val, N_way, n_val, verbose=True)\n",
        "        model.save_weights(os.path.join(model_2_path, 'weights.{}.h5'.format(i)))\n",
        "        if val_acc >= best:\n",
        "            print(\"Current best: {0}, previous best: {1}\".format(val_acc, best))\n",
        "            best = val_acc"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training process!\n",
            "-------------------------------------\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 200 iterations: 0.15213268200556437 mins\n",
            "Train Loss: 0.7059872150421143\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 32.0% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 32.0, previous best: -1\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 400 iterations: 0.5822814504305521 mins\n",
            "Train Loss: 0.5833199620246887\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 36.0% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 36.0, previous best: 32.0\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 600 iterations: 0.9915398955345154 mins\n",
            "Train Loss: 0.6169869303703308\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 48.4% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 48.4, previous best: 36.0\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 800 iterations: 1.4218302249908448 mins\n",
            "Train Loss: 0.5006300210952759\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 40.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 1000 iterations: 1.8278248151143393 mins\n",
            "Train Loss: 0.5681591033935547\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 46.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 1200 iterations: 2.254901921749115 mins\n",
            "Train Loss: 0.5511191487312317\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 51.2% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 51.2, previous best: 48.4\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 1400 iterations: 2.664763867855072 mins\n",
            "Train Loss: 0.35583359003067017\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 51.2% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 51.2, previous best: 51.2\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 1600 iterations: 3.099626914660136 mins\n",
            "Train Loss: 0.3379535675048828\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 54.0% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 54.0, previous best: 51.2\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 1800 iterations: 3.513097047805786 mins\n",
            "Train Loss: 0.38196009397506714\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 57.6% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 57.6, previous best: 54.0\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 2000 iterations: 3.945629533131917 mins\n",
            "Train Loss: 0.32728201150894165\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 55.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 2200 iterations: 4.354565544923147 mins\n",
            "Train Loss: 0.2989656925201416\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 61.2% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 61.2, previous best: 57.6\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 2400 iterations: 4.78056318362554 mins\n",
            "Train Loss: 0.3715244233608246\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 54.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 2600 iterations: 5.191739284992218 mins\n",
            "Train Loss: 0.278793066740036\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 60.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 2800 iterations: 5.62251318693161 mins\n",
            "Train Loss: 0.32091012597084045\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 63.2% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 63.2, previous best: 61.2\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 3000 iterations: 6.0327406287193295 mins\n",
            "Train Loss: 0.4277653694152832\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 66.4% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 66.4, previous best: 63.2\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 3200 iterations: 6.461253770192465 mins\n",
            "Train Loss: 0.23814353346824646\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 57.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 3400 iterations: 6.875580100218455 mins\n",
            "Train Loss: 0.2558363080024719\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 61.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 3600 iterations: 7.303261566162109 mins\n",
            "Train Loss: 0.2319697141647339\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 65.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 3800 iterations: 7.709732552369435 mins\n",
            "Train Loss: 0.26221880316734314\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 65.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 4000 iterations: 8.135930387179057 mins\n",
            "Train Loss: 0.24222326278686523\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 60.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 4200 iterations: 8.550122205416361 mins\n",
            "Train Loss: 0.1979971081018448\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 71.2% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 71.2, previous best: 66.4\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 4400 iterations: 8.977618873119354 mins\n",
            "Train Loss: 0.35612303018569946\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 63.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 4600 iterations: 9.383091207345327 mins\n",
            "Train Loss: 0.23416435718536377\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 60.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 4800 iterations: 9.813288505872091 mins\n",
            "Train Loss: 0.1674797683954239\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 70.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 5000 iterations: 10.224830909570057 mins\n",
            "Train Loss: 0.19676458835601807\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 67.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 5200 iterations: 10.646832092603047 mins\n",
            "Train Loss: 0.18846209347248077\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 65.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 5400 iterations: 11.057543841997783 mins\n",
            "Train Loss: 0.2818886637687683\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 63.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 5600 iterations: 11.481565582752228 mins\n",
            "Train Loss: 0.247218057513237\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 64.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 5800 iterations: 11.898460817337035 mins\n",
            "Train Loss: 0.18085792660713196\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 66.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 6000 iterations: 12.324250888824462 mins\n",
            "Train Loss: 0.2506701946258545\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 75.2% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 75.2, previous best: 71.2\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 6200 iterations: 12.738096487522125 mins\n",
            "Train Loss: 0.14078578352928162\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 68.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 6400 iterations: 13.162670318285624 mins\n",
            "Train Loss: 0.18501931428909302\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 75.6% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 75.6, previous best: 75.2\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 6600 iterations: 13.575421226024627 mins\n",
            "Train Loss: 0.3393916189670563\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 70.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 6800 iterations: 14.000598069032034 mins\n",
            "Train Loss: 0.15457503497600555\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 72.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 7000 iterations: 14.416296247641245 mins\n",
            "Train Loss: 0.2067510485649109\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 74.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 7200 iterations: 14.839155387878417 mins\n",
            "Train Loss: 0.13994571566581726\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 70.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 7400 iterations: 15.25526538292567 mins\n",
            "Train Loss: 0.1169201135635376\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 71.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 7600 iterations: 15.674684766928355 mins\n",
            "Train Loss: 0.1385353058576584\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 57.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 7800 iterations: 16.088663029670716 mins\n",
            "Train Loss: 0.19239507615566254\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 70.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 8000 iterations: 16.514120745658875 mins\n",
            "Train Loss: 0.09780538827180862\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 71.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 8200 iterations: 16.930749535560608 mins\n",
            "Train Loss: 0.18965721130371094\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 64.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 8400 iterations: 17.351231726010642 mins\n",
            "Train Loss: 0.08757729828357697\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 78.0% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 78.0, previous best: 75.6\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 8600 iterations: 17.767015977700552 mins\n",
            "Train Loss: 0.18426281213760376\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 65.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 8800 iterations: 18.195927588144936 mins\n",
            "Train Loss: 0.21673628687858582\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 68.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 9000 iterations: 18.61604716380437 mins\n",
            "Train Loss: 0.13546863198280334\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 72.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 9200 iterations: 19.04171090523402 mins\n",
            "Train Loss: 0.23662415146827698\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 62.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 9400 iterations: 19.4553662776947 mins\n",
            "Train Loss: 0.1397787481546402\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 64.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 9600 iterations: 19.881031600634255 mins\n",
            "Train Loss: 0.0989585891366005\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 69.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 9800 iterations: 20.29251116514206 mins\n",
            "Train Loss: 0.16924409568309784\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 69.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 10000 iterations: 20.715877827008566 mins\n",
            "Train Loss: 0.12873630225658417\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 64.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 10200 iterations: 21.13061213493347 mins\n",
            "Train Loss: 0.1530579924583435\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 72.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 10400 iterations: 21.553552397092183 mins\n",
            "Train Loss: 0.12203887104988098\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 77.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 10600 iterations: 21.967216606934866 mins\n",
            "Train Loss: 0.1358432173728943\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 73.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 10800 iterations: 22.395215932528178 mins\n",
            "Train Loss: 0.11508463323116302\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 75.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 11000 iterations: 22.809853772322338 mins\n",
            "Train Loss: 0.09993110597133636\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 79.6% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 79.6, previous best: 78.0\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 11200 iterations: 23.234556810061136 mins\n",
            "Train Loss: 0.10874871909618378\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 71.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 11400 iterations: 23.650474417209626 mins\n",
            "Train Loss: 0.18140541017055511\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 60.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 11600 iterations: 24.07926003932953 mins\n",
            "Train Loss: 0.13805246353149414\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 53.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 11800 iterations: 24.491719738642374 mins\n",
            "Train Loss: 0.11949855089187622\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 73.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 12000 iterations: 24.916461698214214 mins\n",
            "Train Loss: 0.11592000722885132\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 76.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 12200 iterations: 25.32971680164337 mins\n",
            "Train Loss: 0.13581600785255432\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 80.0% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 80.0, previous best: 79.6\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 12400 iterations: 25.7551655848821 mins\n",
            "Train Loss: 0.21859928965568542\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 77.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 12600 iterations: 26.167228798071545 mins\n",
            "Train Loss: 0.14681656658649445\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 68.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 12800 iterations: 26.59188648064931 mins\n",
            "Train Loss: 0.11074507981538773\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 65.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 13000 iterations: 27.00650225877762 mins\n",
            "Train Loss: 0.16253885626792908\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 82.4% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 82.4, previous best: 80.0\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 13200 iterations: 27.433167906602225 mins\n",
            "Train Loss: 0.1268644630908966\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 74.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 13400 iterations: 27.847696435451507 mins\n",
            "Train Loss: 0.11043960601091385\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 74.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 13600 iterations: 28.27674601872762 mins\n",
            "Train Loss: 0.14693425595760345\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 78.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 13800 iterations: 28.689820766448975 mins\n",
            "Train Loss: 0.12247355282306671\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 68.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 14000 iterations: 29.117775841554007 mins\n",
            "Train Loss: 0.0845833271741867\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 78.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 14200 iterations: 29.53295110066732 mins\n",
            "Train Loss: 0.09921351820230484\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 69.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 14400 iterations: 29.960248879591624 mins\n",
            "Train Loss: 0.10422319173812866\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 79.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 14600 iterations: 30.378363537788392 mins\n",
            "Train Loss: 0.14328816533088684\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 76.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 14800 iterations: 30.810047801335653 mins\n",
            "Train Loss: 0.11695195734500885\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 68.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 15000 iterations: 31.224919752279916 mins\n",
            "Train Loss: 0.09028095006942749\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 70.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 15200 iterations: 31.65116263628006 mins\n",
            "Train Loss: 0.1486632525920868\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 78.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 15400 iterations: 32.06826122601827 mins\n",
            "Train Loss: 0.1441904455423355\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 69.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 15600 iterations: 32.49438602129619 mins\n",
            "Train Loss: 0.09639304876327515\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 73.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 15800 iterations: 32.91178228457769 mins\n",
            "Train Loss: 0.10342331230640411\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 68.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 16000 iterations: 33.33673164844513 mins\n",
            "Train Loss: 0.11473511159420013\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 76.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 16200 iterations: 33.75244219700495 mins\n",
            "Train Loss: 0.11704082787036896\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 78.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 16400 iterations: 34.17735257943471 mins\n",
            "Train Loss: 0.09899038076400757\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 71.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 16600 iterations: 34.59169233640035 mins\n",
            "Train Loss: 0.12523458898067474\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 65.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 16800 iterations: 35.01956427494685 mins\n",
            "Train Loss: 0.10691113024950027\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 69.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 17000 iterations: 35.43140217463176 mins\n",
            "Train Loss: 0.10090802609920502\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 67.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 17200 iterations: 35.8548646291097 mins\n",
            "Train Loss: 0.08792389929294586\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 78.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 17400 iterations: 36.268284634749094 mins\n",
            "Train Loss: 0.09711474180221558\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 72.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 17600 iterations: 36.69436678489049 mins\n",
            "Train Loss: 0.11076454818248749\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 70.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 17800 iterations: 37.112449777126315 mins\n",
            "Train Loss: 0.10583315044641495\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 77.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 18000 iterations: 37.53708382050196 mins\n",
            "Train Loss: 0.09488525986671448\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 71.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 18200 iterations: 37.95115305980047 mins\n",
            "Train Loss: 0.09115588665008545\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 85.2% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 85.2, previous best: 82.4\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 18400 iterations: 38.37230033874512 mins\n",
            "Train Loss: 0.16184815764427185\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 82.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 18600 iterations: 38.78214625914892 mins\n",
            "Train Loss: 0.09463004767894745\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 81.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 18800 iterations: 39.20879069566727 mins\n",
            "Train Loss: 0.09700874239206314\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 70.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 19000 iterations: 39.623571407794955 mins\n",
            "Train Loss: 0.08530648052692413\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 83.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 19200 iterations: 40.04915061394374 mins\n",
            "Train Loss: 0.08914446085691452\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 77.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 19400 iterations: 40.46326402425766 mins\n",
            "Train Loss: 0.07367577403783798\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 71.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 19600 iterations: 40.8894544561704 mins\n",
            "Train Loss: 0.13150537014007568\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 78.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 19800 iterations: 41.30209529002507 mins\n",
            "Train Loss: 0.08605093508958817\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 74.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 20000 iterations: 41.73019851446152 mins\n",
            "Train Loss: 0.11973847448825836\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 81.6% 20 way one-shot learning accuracy \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4JoCm_5jbDk"
      },
      "source": [
        "# print(\"Starting training process!\")\n",
        "# print(\"-------------------------------------\")\n",
        "# t_start = time.time()\n",
        "# for i in range(1, n_iter+1):\n",
        "#     (inputs,targets) = createPairs(X,y,sizes,batch_size)\n",
        "#     loss = model.train_on_batch(inputs, targets)\n",
        "#     if i % evaluate_every == 0:\n",
        "#         print(\"\\n ------------- \\n\")\n",
        "#         print(\"Time for {0} iterations: {1} mins\".format(i, (time.time()-t_start)/60.0))\n",
        "#         print(\"Train Loss: {0}\".format(loss)) \n",
        "#         val_acc = test_oneshot(model,Xval,yval,sizes_val, N_way, n_val, verbose=True)\n",
        "#         model.save_weights(os.path.join(model_path, 'weights.{}.h5'.format(i)))\n",
        "#         if val_acc >= best:\n",
        "#             print(\"Current best: {0}, previous best: {1}\".format(val_acc, best))\n",
        "#             best = val_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcTZ3HHJjbDp"
      },
      "source": [
        "## loading model from weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yt8pqshBxKcQ"
      },
      "source": [
        "# model_path="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XSzGDRhjbDr"
      },
      "source": [
        "model.load_weights(model_path+'weights.20000.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFJVIYiajbDs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDQXUEmFjbDt"
      },
      "source": [
        "def calc_accuracy(N,Xval,yval,anchor_img,anchor_label,model):\n",
        "    count_first=0\n",
        "    count_first3=0\n",
        "    for i in range(N):\n",
        "        ind=random.choice(range(yval.shape[0]))\n",
        "        predicted,anchor_imgs,targets=whichGlyph(model,Xval[ind],anchor_img,anchor_label)\n",
        "        sort_index = np.argsort(np.asarray(predicted).reshape(len(predicted),))\n",
        "        if targets[sort_index[-1]] == yval[ind][0]:\n",
        "            count_first+=1\n",
        "        if yval[ind][0] in targets[sort_index[127:]]:\n",
        "            count_first3+=1\n",
        "    accuracy_first=count_first/N\n",
        "    accuracy_first3=count_first3/N\n",
        "    \n",
        "    return accuracy_first, accuracy_first3"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmLQ6lVYjbDt",
        "outputId": "5a69c840-d17a-43c3-b42c-18c32d4a3895"
      },
      "source": [
        "t_start = time.time()\n",
        "acc1,acc3=calc_accuracy(250,Xval,yval,anchor_img,anchor_label,model)\n",
        "print(f'testing:\\nfound first accuracy = {acc1} , first 3 accuracy = {acc3} ')\n",
        "print(\"accuracy fn took {0} mins\".format((time.time()-t_start)/60.0))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "testing:\n",
            "found first accuracy = 0.688 , first 3 accuracy = 0.972 \n",
            "accuracy fn took 0.4506264050801595 mins\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdtRB3qAjbDw",
        "outputId": "24395599-0295-42e0-ad35-ffaccc1606f6"
      },
      "source": [
        "t_start = time.time()\n",
        "acc1,acc3=calc_accuracy(250,X,y,anchor_img,anchor_label,model)\n",
        "print(f'training:\\nfound first accuracy = {acc1} , first 3 accuracy = {acc3} ')\n",
        "print(\"accuracy fn took {0} mins\".format((time.time()-t_start)/60.0))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training:\n",
            "found first accuracy = 0.82 , first 3 accuracy = 0.992 \n",
            "accuracy fn took 0.447417942682902 mins\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poAicP_njbDx",
        "outputId": "023c1077-47e8-4204-88f0-909a0d087974"
      },
      "source": [
        "# t_start = time.time()\n",
        "# print(calc_accuracy(1,Xval,yval,anchor_img,anchor_label,model))\n",
        "# print(\"accuracy fn took {0} sec\".format((time.time()-t_start)))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1.0, 1.0)\n",
            "accuracy fn took 0.10917329788208008 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beW1a29IjbDz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrLEK0fejbDz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZplSFXGjbD3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}