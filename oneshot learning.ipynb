{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "oneshot learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9EUfmmBja-_"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import skimage.io as io\n",
        "import pickle\n",
        "\n",
        "import time\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
        "from keras.models import Model\n",
        "\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.layers.core import Lambda, Flatten, Dense\n",
        "from keras.initializers import glorot_uniform\n",
        "\n",
        "from tensorflow.keras.layers import Layer\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "# from tensorflow.keras.preprocessing import image\n",
        "# from tensorflow.keras.models import Model\n",
        "# from tensorflow.keras.layers import Dense, GlobalAveragePooling2D"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXwbeZYYjbAJ"
      },
      "source": [
        "## Loading data\n",
        "a function that loads the images locations and labels.<br>\n",
        "input: the data path.<br>\n",
        "output:<br>\n",
        "&emsp;&emsp;dataHiero -> a dataframe with index= location of images and label= their labels <br>\n",
        "&emsp;&emsp;img_groups -> a dictionary in the shape of { \"label\" : [array of locations of images labeled with this label] }"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noWbwGlLjbAY"
      },
      "source": [
        "path=\"../GlyphDataset/Dataset/Manual/Preprocessed/\"\n",
        "\n",
        "def loadData(folderPictures=path):\n",
        "    \n",
        "    folders=next(os.walk(folderPictures))[1]\n",
        "    img_groups = {}\n",
        "    img_list={}\n",
        "\n",
        "    for folder in folders:\n",
        "        for img_file in os.listdir(folderPictures+folder):\n",
        "            name, \n",
        "            label = img_file.strip('.png').split(\"_\")\n",
        "            \n",
        "            \n",
        "            # One image per class\n",
        "\n",
        "            #if label not in img_groups.keys():\n",
        "            #    img_groups[label] = [folder + \"_\" + name]\n",
        "\n",
        "\n",
        "            # Multiple images per class\n",
        "\n",
        "            if label in img_groups.keys():\n",
        "                img_groups[label].append(folder+\"_\"+name)\n",
        "            else:\n",
        "                img_groups[label] = [folder+\"_\"+name]\n",
        "\n",
        "            img_list[folder+\"_\"+name]=[label]\n",
        "\n",
        "\n",
        "    # Remove class with only one hieroglyph\n",
        "\n",
        "\n",
        "    for k,v in list(img_groups.items()):\n",
        "        if len(v)==1: del img_groups[k]\n",
        "\n",
        "    # Extract only N hieroglyph classes randomly\n",
        "\n",
        "    nclass = len(img_groups.keys())\n",
        "\n",
        "    list_of_class = random.sample(list(img_groups.keys()), nclass)\n",
        "#     print(list_of_class)\n",
        "\n",
        "    short_dico = {x: img_groups[x] for x in list_of_class if x in img_groups}\n",
        "\n",
        "    dataHiero=pd.DataFrame.from_dict(img_list,orient='index')\n",
        "    dataHiero.columns = [\"label\"]\n",
        "    dataHiero = dataHiero[dataHiero.label != 'UNKNOWN']\n",
        "\n",
        "    dataHiero = dataHiero.loc[dataHiero['label'].isin(short_dico)]\n",
        "\n",
        "\n",
        "    dataHiero.reset_index(level=0, inplace=True)\n",
        "\n",
        "    return dataHiero,img_groups"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CaZM8DxjbAp"
      },
      "source": [
        "a function that takes the image groups and load those images<br>\n",
        "input: img_proups dictionary<br>\n",
        "output:<br>\n",
        "&emsp;&emsp;X -> np array of the images<br>\n",
        "&emsp;&emsp;y -> np array of labels<br>\n",
        "&emsp;&emsp;glyph_sizes -> a dictionary in the form of {'label' : (starting index, ending index in X and y)}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO2UaDgTjbAu"
      },
      "source": [
        "def read_images(img_groups,path):\n",
        "    X=[]\n",
        "    y=[]\n",
        "    glyph_sizes={}\n",
        "    low=0\n",
        "    for glyph in img_groups:\n",
        "        category_images=[]\n",
        "        high=low\n",
        "        for img_path in img_groups[glyph] :\n",
        "            folder,name = img_path.split('_')\n",
        "            image = io.imread(path+folder+'/'+name+'_'+glyph+'.png')\n",
        "            X.append(image)\n",
        "            y.append(glyph)\n",
        "            high+=1\n",
        "#         X.append(np.array(category_images))\n",
        "        glyph_sizes[glyph]=(low,high-1)\n",
        "        low=high\n",
        "        \n",
        "    return np.array(X),np.array(y).reshape((-1,1)),glyph_sizes\n",
        "            \n",
        "    "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYwf13e4jbA3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "outputId": "40164a56-2bf4-4e9f-9e17-823f4146688c"
      },
      "source": [
        "dataHiero,img_groups=loadData(folderPictures=path)\n",
        "dataHiero.head()\n",
        "# img_groups"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "StopIteration",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-4ff87f925fef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataHiero\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_groups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolderPictures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdataHiero\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# img_groups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-62ed25224a45>\u001b[0m in \u001b[0;36mloadData\u001b[0;34m(folderPictures)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolderPictures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mfolders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolderPictures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mimg_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mimg_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mStopIteration\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-j6RWKkrjbA6"
      },
      "source": [
        "X,y,sizes=read_images(img_groups,path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "kNTAe8oQjbBG",
        "outputId": "a030e446-b195-4481-b03e-2b201b17045f"
      },
      "source": [
        "type(X)\n",
        "print(y.shape)\n",
        "print(X.shape)\n",
        "sizes['D21'][1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2921, 1)\n",
            "(2921, 75, 50)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjQdogROjbBI"
      },
      "source": [
        "saving the images into a pickle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br2-Xm7bjbBb"
      },
      "source": [
        "#train val split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,stratify=y, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPS8xCJPjbBh"
      },
      "source": [
        "def get_sizes(X,Y):\n",
        "    sizes={}\n",
        "    for i ,(x,y) in enumerate(zip(X,Y)):\n",
        "#         print(i,x,y)\n",
        "        if y[0] in sizes:\n",
        "            sizes[y[0]].append(i)\n",
        "        else:\n",
        "            sizes[y[0]]=[i]\n",
        "    return sizes\n",
        "    "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sG1c9UUHjbBi"
      },
      "source": [
        "sizes=get_sizes(X_train,y_train)\n",
        "X=X_train\n",
        "y=y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbrU-fAkjbBn"
      },
      "source": [
        "#saving data as pickle\n",
        "with open(\"train.pickle\", \"wb\") as f:\n",
        "    pickle.dump((X,y,sizes),f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4szlW2XjbBo"
      },
      "source": [
        "sizes_val=get_sizes(X_test,y_test)\n",
        "Xval=X_test\n",
        "yval=y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkrX6IOujbBp"
      },
      "source": [
        "#saving data as pickle\n",
        "with open(\"test.pickle\", \"wb\") as f:\n",
        "    pickle.dump((Xval,yval,sizes_val),f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUfovN9vjbBw"
      },
      "source": [
        "# #saving data as pickle\n",
        "# with open(\"test.pickle\", \"wb\") as f:\n",
        "#     pickle.dump((Xval,yval,sizes_val),f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtd7kUY6jbBx"
      },
      "source": [
        "## reading the training tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVz6vsHZ4SeJ"
      },
      "source": [
        "#colab\n",
        "data_path= '/content/drive/MyDrive/hiero_cv/'\n",
        "#local\n",
        "# data_path='./'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k19Gi0fR4BU9",
        "outputId": "cdf0dd13-2c96-4c62-abce-bd11e35dd843"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZPC6wKrjbB4"
      },
      "source": [
        "with open(data_path+\"train.pickle\", \"rb\") as f:\n",
        "    (X,y,sizes) = pickle.load(f)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOstoN_vjbB7"
      },
      "source": [
        "with open(data_path+\"test.pickle\", \"rb\") as f:\n",
        "    (Xval,yval,sizes_val) = pickle.load(f)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmdcIQiwpDLV",
        "outputId": "0d44f9a1-1115-4f46-df94-5903a8bde210"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOB0W_VqjbB8"
      },
      "source": [
        "def initialize_weights(shape, name=None):\n",
        "    \"\"\"\n",
        "        The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
        "        suggests to initialize CNN layer weights with mean as 0.0 and standard deviation of 0.01\n",
        "    \"\"\"\n",
        "    return np.random.normal(loc = 0.0, scale = 1e-2, size = shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xwz8vTAmjbCB"
      },
      "source": [
        "def initialize_bias(shape, name=None):\n",
        "    \"\"\"\n",
        "        The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
        "        suggests to initialize CNN layer bias with mean as 0.5 and standard deviation of 0.01\n",
        "    \"\"\"\n",
        "    return np.random.normal(loc = 0.5, scale = 1e-2, size = shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xJN3eoVjbCQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G63fmh8-jbCR"
      },
      "source": [
        "# def get_siamese_model_2(input_shape):\n",
        "#     \"\"\"\n",
        "#         Model architecture based on the one provided in: http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
        "#     \"\"\"\n",
        "    \n",
        "#     # Define the tensors for the two input images\n",
        "#     left_input = Input(input_shape)\n",
        "#     right_input = Input(input_shape)\n",
        "    \n",
        "#     # Convolutional Neural Network\n",
        "#     model = Sequential()\n",
        "#     model.add(Conv2D(64, (3,3),strides=(2, 2), activation='relu', input_shape=input_shape, kernel_regularizer=l2(2e-4)))\n",
        "#     model.add(MaxPooling2D((2, 2), strides=2))\n",
        "#     model.add(Conv2D(64, (3,3), activation='relu', kernel_regularizer=l2(2e-4)))\n",
        "#     model.add(MaxPooling2D((2, 2), strides=2))\n",
        "#     model.add(Conv2D(128, (3,3), activation='relu', kernel_regularizer=l2(2e-4)))\n",
        "# #     model.add(MaxPooling2D())\n",
        "# #     model.add(Conv2D(256, (4,4), activation='relu',  kernel_regularizer=l2(2e-4)))\n",
        "#     model.add(Flatten())\n",
        "#     model.add(Dense(4096, activation='sigmoid',\n",
        "#                    kernel_regularizer=l2(1e-3)))\n",
        "    \n",
        "#     # Generate the encodings (feature vectors) for the two images\n",
        "#     encoded_l = model(left_input)\n",
        "#     encoded_r = model(right_input)\n",
        "    \n",
        "#     # Add a customized layer to compute the absolute difference between the encodings\n",
        "#     L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
        "#     L1_distance = L1_layer([encoded_l, encoded_r])\n",
        "    \n",
        "#     # Add a dense layer with a sigmoid unit to generate the similarity score\n",
        "#     prediction = Dense(1,activation='sigmoid')(L1_distance)\n",
        "    \n",
        "#     # Connect the inputs with the outputs\n",
        "#     siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
        "    \n",
        "#     # return the model\n",
        "#     return siamese_net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "pLzzLePsjbCS"
      },
      "source": [
        "# model_2 = get_siamese_model_2((75, 50, 1))\n",
        "# model_2.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk2Tsft9jbCT"
      },
      "source": [
        "# optimizer = Adam(lr = 0.001)\n",
        "# model_2.compile(loss=\"binary_crossentropy\",optimizer=optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFcNXLnxjbCT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zodoNhCYjbCU"
      },
      "source": [
        "# def transfer_model(input_shape):\n",
        "#     \"\"\"\n",
        "#         Model architecture based on the one provided in: http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
        "#     \"\"\"\n",
        "    \n",
        "#     # Define the tensors for the two input images\n",
        "#     left_input = Input(input_shape)\n",
        "#     right_input = Input(input_shape)\n",
        "    \n",
        "#     #Import inception model for transfer learning without output layers\n",
        "#     base_model = InceptionV3(weights='imagenet', include_top=False, input_shape = input_shape)\n",
        "    \n",
        "    \n",
        "#     x = base_model.output\n",
        "#     x = GlobalAveragePooling2D()(x)\n",
        "#     # let's add a fully-connected layer\n",
        "#     x = Dense(1024, activation='relu')(x)\n",
        "#     # and a logistic layer -- let's say we have 200 classes\n",
        "#     model= Dense(200, activation='softmax')(x)\n",
        "    \n",
        "#     # this is the model we will train\n",
        "# #     model = Model(inputs=base_model.input, outputs=predictions)\n",
        "        \n",
        "    \n",
        "#     # Generate the encodings (feature vectors) for the two images\n",
        "#     encoded_l = model(left_input)\n",
        "#     encoded_r = model(right_input)\n",
        "    \n",
        "#     # Add a customized layer to compute the absolute difference between the encodings\n",
        "#     L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
        "#     L1_distance = L1_layer([encoded_l, encoded_r])\n",
        "    \n",
        "#     # Add a dense layer with a sigmoid unit to generate the similarity score\n",
        "#     prediction = Dense(1,activation='sigmoid')(L1_distance)\n",
        "    \n",
        "#     # Connect the inputs with the outputs\n",
        "#     model = Model(inputs=[left_input,right_input],outputs=prediction)\n",
        "    \n",
        "#     for layer in model.layers[:249]:\n",
        "#        layer.trainable = False\n",
        "#     for layer in model.layers[249:]:\n",
        "#        layer.trainable = True\n",
        "    \n",
        "#     # return the model\n",
        "#     return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-fIRWtajbCV"
      },
      "source": [
        "# inception_model = transfer_model((75, 50, 1))\n",
        "# inception_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEfmyM_KjbCW"
      },
      "source": [
        "# inception_model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRlqnliu5So9"
      },
      "source": [
        "from keras import backend as K"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpR7qVOCjbCX"
      },
      "source": [
        "def get_siamese_model(input_shape):\n",
        "    \"\"\"\n",
        "        Model architecture based on the one provided in: http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define the tensors for the two input images\n",
        "    left_input = Input(input_shape)\n",
        "    right_input = Input(input_shape)\n",
        "    \n",
        "    # Convolutional Neural Network\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(64, (3,3), activation='relu', input_shape=input_shape, kernel_regularizer=l2(2e-4)))\n",
        "    model.add(MaxPooling2D())\n",
        "    model.add(Conv2D(64, (3,3), activation='relu', kernel_regularizer=l2(2e-4)))\n",
        "    model.add(MaxPooling2D())\n",
        "    model.add(Conv2D(128, (4,4), activation='relu', kernel_regularizer=l2(2e-4)))\n",
        "    model.add(MaxPooling2D())\n",
        "    model.add(Conv2D(256, (4,4), activation='relu',  kernel_regularizer=l2(2e-4)))\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(4096, activation='sigmoid',\n",
        "                   kernel_regularizer=l2(1e-3)))\n",
        "    \n",
        "    # Generate the encodings (feature vectors) for the two images\n",
        "    encoded_l = model(left_input)\n",
        "    encoded_r = model(right_input)\n",
        "    \n",
        "    # Add a customized layer to compute the absolute difference between the encodings\n",
        "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
        "    \n",
        "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
        "    L1_distance = Dense(256,activation='relu')(L1_distance)\n",
        "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
        "    prediction = Dense(1,activation='sigmoid')(L1_distance)\n",
        "\n",
        "    #cosine similarity\n",
        "    # def cosine_distance(vests):\n",
        "    #   x, y = vests\n",
        "    #   x = K.l2_normalize(x, axis=-1)\n",
        "    #   y = K.l2_normalize(y, axis=-1)\n",
        "    #   return -K.mean(x * y, axis=-1, keepdims=True)\n",
        "    \n",
        "    \n",
        "    # prediction = Lambda(cosine_distance, output_shape=1)([encoded_l, encoded_r])\n",
        "    # Connect the inputs with the outputs\n",
        "    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
        "    \n",
        "    # return the model\n",
        "    return siamese_net"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MtvWYCyjbCb",
        "outputId": "1344ab99-6a02-4714-bed6-6114f6b98321"
      },
      "source": [
        "model = get_siamese_model((75, 50, 1))\n",
        "model.summary()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_9 (InputLayer)            [(None, 75, 50, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_10 (InputLayer)           [(None, 75, 50, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential_4 (Sequential)       (None, 4096)         4891712     input_9[0][0]                    \n",
            "                                                                 input_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_5 (Lambda)               (None, 4096)         0           sequential_4[0][0]               \n",
            "                                                                 sequential_4[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 256)          1048832     lambda_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 1)            257         dense_6[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 5,940,801\n",
            "Trainable params: 5,940,801\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpDd_9CWjbCd",
        "outputId": "390bfbc4-9530-4b9a-8e0c-cb79745ab9aa"
      },
      "source": [
        "optimizer = Adam(lr = 0.0001)\n",
        "model.compile(loss=\"binary_crossentropy\",optimizer=optimizer)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNSpnQaljbCe"
      },
      "source": [
        "a function that create pairs of images with y= 1 if they are similar and 0 if they are different. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUBHJas8jbCf"
      },
      "source": [
        "## a function to predict which glyph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kua3lTs3jbCf"
      },
      "source": [
        "def create_glyphlist(X,sizes):\n",
        "    images=[]\n",
        "    labels=[]\n",
        "    _,w,h=X.shape\n",
        "    for glyph in sizes:\n",
        "        index=sizes[glyph][0]\n",
        "        images.append(X[index].reshape( w , h, 1))\n",
        "        labels.append(glyph)\n",
        "    return np.asarray(images), np.asarray(labels) "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTWwXWyBjbCn"
      },
      "source": [
        "anchor_img, anchor_label=create_glyphlist(X,sizes)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UV5F8TRwjbCo"
      },
      "source": [
        ""
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cj5krLHkjbCo"
      },
      "source": [
        "def whichGlyph_pair(image,anchor_img,anchor_label):\n",
        "    N,w,h,_=anchor_img.shape\n",
        "#     pairs=[np.zeros((N, w, h,1)) for i in range(2)]\n",
        "    \n",
        "    test_image= np.asarray([image]*N).reshape(N, w, h,1)\n",
        "    \n",
        "    anchor_label, test_image, anchor_img = shuffle(anchor_label, test_image, anchor_img)\n",
        "#     pairs = [test_image,anchor_img]\n",
        "    \n",
        "    return test_image, anchor_img, anchor_label\n",
        "    "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzVYcJ4tjbCp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrc8caDFjbCp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6a5mx_MjbCq"
      },
      "source": [
        "def whichGlyph(model,image,anchor_img,anchor_label):\n",
        "    test_image,anchor_img,targets = whichGlyph_pair(image,anchor_img,anchor_label)\n",
        "    probs = model.predict([test_image,anchor_img])\n",
        "    return probs,anchor_img,targets\n",
        "    "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUxUIQb8jbC1"
      },
      "source": [
        "# def whichGlyph(model,image,anchor_img,anchor_label):\n",
        "#     test_image,anchor_img,targets = whichGlyph_pair(image,anchor_img,anchor_label)\n",
        "#     probs=[]\n",
        "#     for i in range(0,len(targets),2):\n",
        "#         pair=[[test_image[i],test_image[i+1]],[anchor_img[i],anchor_img[i+1]]]\n",
        "#         pred=model.predict(pair)\n",
        "#         for p in pred:\n",
        "#             probs.append(p)\n",
        "        \n",
        "#     return probs,anchor_img,targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kb2Uo1PrjbC2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arWKCACTjbC4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2SsLM6ujbC5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eM5fQribjbC5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xCa5bCWjbC6"
      },
      "source": [
        "## creating pairs of images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7t5J2GCjbC7"
      },
      "source": [
        "def createPairs(X,y,sizes,batch_size):\n",
        "    ##create a batch with half it's size are similar glyphs and the other half are different.\n",
        "    n=0\n",
        "    i=0\n",
        "    \n",
        "    label=[]\n",
        "    _,w,h=X.shape\n",
        "    # initialize 2 empty arrays for the input image batch\n",
        "#     pairs=[np.zeros((batch_size, w, h,1)) for i in range(2)]\n",
        "    input1=np.zeros((batch_size, w, h,1))\n",
        "    input2=np.zeros((batch_size, w, h,1))\n",
        "    \n",
        "    while n < batch_size:\n",
        "        random_key1=random.choice(list(sizes))\n",
        "#         low=sizes[random_key1][0]\n",
        "#         high=sizes[random_key1][1]\n",
        "        index1, index3 = np.random.choice(sizes[random_key1], size=2)\n",
        "        index2 = np.random.choice(sizes[random_key1])\n",
        "        random_key2=random.choice(list(sizes))\n",
        "        \n",
        "        while random_key2 == random_key1:\n",
        "            random_key2=random.choice(list(sizes))\n",
        "            \n",
        "#         low=sizes[random_key2][0]\n",
        "#         high=sizes[random_key2][1]\n",
        "        index4=np.random.choice(sizes[random_key2])\n",
        "        n += 2\n",
        "        # appending images 1 and 3 into input1 and input2 corresponding to y=1 \n",
        "        #and images 2 and 4 corresponding to y=0\n",
        "    \n",
        "        input1[i,:,:,:] = X[index1].reshape( w , h, 1)\n",
        "        input1[i+1,:,:,:] = X[index2].reshape(w, h, 1)\n",
        "        input2[i,:,:,:] = X[index3].reshape(w, h, 1)\n",
        "        input2[i+1,:,:,:] = X[index4].reshape(w, h, 1)\n",
        "        i += 2\n",
        "#         input1+=[X[index1],X[index2]]\n",
        "#         input2+=[X[index3],X[index4]]\n",
        "        label+=[1,0]\n",
        "        \n",
        "#         print(index1,index2,index3,index4)\n",
        "#         print(y[index1],y[index2],y[index3],y[index4])\n",
        "#         print(random_key1,random_key2)\n",
        "    input1,input2,label = shuffle(input1,input2,label)\n",
        "    pairs=[input1,input2]\n",
        "    \n",
        "    return pairs,label\n",
        "pairs,label=createPairs(X,y,sizes,32)   "
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXoehBUTjbC9"
      },
      "source": [
        "# pairs[1].shape\n",
        "# label"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alqGHaxLjbC-"
      },
      "source": [
        "a fn that creates a N-way one shot learning task where it create pairs with the wanted image and N-1 different ones and 1 similar one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVWV2CnbjbC-"
      },
      "source": [
        "def make_oneshot_task(X,y,sizes,N):\n",
        "    _,w,h=X.shape\n",
        "    pairs=[np.zeros((N, w, h,1)) for i in range(2)]\n",
        "    \n",
        "    true_key=random.choice(list(sizes))\n",
        "#     low=sizes[true_key][0]\n",
        "#     high=sizes[true_key][1]\n",
        "    \n",
        "    index=np.random.choice(sizes[true_key])\n",
        "    index2=np.random.choice(sizes[true_key])\n",
        "    \n",
        "    test_image= np.asarray([X[index]]*N).reshape(N, w, h,1)\n",
        "#     print(X.shape)\n",
        "    X_diff= np.delete(X,sizes[true_key],axis=0)\n",
        "#     print(X_diff.shape)\n",
        "    indices= np.random.choice(range(0, len(X_diff)), size = N-1)\n",
        "    \n",
        "    support_set=X_diff[indices,:,:]\n",
        "    ##!!! adding the similar image to the start of the array is not working!!!!!!!!!!!!!!!!!!!!!\n",
        "    \n",
        "#     print(support_set.shape)\n",
        "#     support_set = [ X[index2] ] + support_set\n",
        "    support_set=np.insert(support_set,0,X[index2],axis=0)\n",
        "#     print(support_set.shape)\n",
        "    targets = np.zeros((N,))\n",
        "    targets[0] = 1\n",
        "    support_set=support_set.reshape(N,w,h,1)\n",
        "    targets, test_image, support_set = shuffle(targets, test_image, support_set)\n",
        "    pairs = [test_image,support_set]\n",
        "\n",
        "    return pairs, targets\n",
        "    \n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27BD6Rg9jbDO",
        "outputId": "b07478be-0ea4-401c-c5cc-6f0117d0a430"
      },
      "source": [
        "pp,tt=make_oneshot_task(Xval,yval,sizes_val,20) \n",
        "pp[0].shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 75, 50, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6lenk9bjbDP",
        "outputId": "ef83aa0b-10f5-47da-e0cf-51048b493898"
      },
      "source": [
        "# plt.imshow(pairs[0][0])\n",
        "pairs[1].shape\n",
        "# len(label)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32, 75, 50, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr6bSOuBjbDZ"
      },
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8POHa_JjbDZ"
      },
      "source": [
        "def test_oneshot(model,X,y,sizes, N, k, s = \"val\", verbose = 0):\n",
        "    \"\"\"Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks\"\"\"\n",
        "    n_correct = 0\n",
        "    if verbose:\n",
        "        print(\"Evaluating model on {} random {} way one-shot learning tasks ... \\n\".format(k,N))\n",
        "    for i in range(k):\n",
        "        inputs, targets = make_oneshot_task(X,y,sizes,N)\n",
        "        probs = model.predict(inputs)\n",
        "        if np.argmax(probs) == np.argmax(targets):\n",
        "            n_correct+=1\n",
        "    percent_correct = (100.0 * n_correct / k)\n",
        "    if verbose:\n",
        "        print(\"Got an average of {}% {} way one-shot learning accuracy \\n\".format(percent_correct,N))\n",
        "    return percent_correct"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbUqy4FNjbDf"
      },
      "source": [
        "# Hyper parameters\n",
        "evaluate_every = 200 # interval for evaluating on one-shot tasks\n",
        "batch_size = 32\n",
        "n_iter = 20000 # No. of training iterations 20000\n",
        "N_way = 20 # how many classes for testing one-shot tasks\n",
        "n_val = 250 # how many one-shot tasks to validate on\n",
        "best = -1"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJXH6WomjbDg"
      },
      "source": [
        "model_path = './weights/'\n",
        "model_2_path= '/content/drive/MyDrive/hiero_cv/weights/'\n",
        "model_cos_path= '/content/drive/MyDrive/hiero_cv/weights_cos/'"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUJcfqJLjbDh",
        "outputId": "f9789a60-ba83-47b0-b60b-08942bf2689c"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2921, 75, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwLw68YljbDj",
        "outputId": "2b663216-cd1c-408e-f1ab-b1a758f23866"
      },
      "source": [
        "print(\"Starting training process!\")\n",
        "print(\"-------------------------------------\")\n",
        "t_start = time.time()\n",
        "for i in range(1, n_iter+1):\n",
        "    (inputs,targets) = createPairs(X,y,sizes,batch_size)\n",
        "    targets=np.asarray(targets)\n",
        "    loss = model.train_on_batch(inputs, targets)\n",
        "    if i % evaluate_every == 0:\n",
        "        print(\"\\n ------------- \\n\")\n",
        "        print(\"Time for {0} iterations: {1} mins\".format(i, (time.time()-t_start)/60.0))\n",
        "        print(\"Train Loss: {0}\".format(loss)) \n",
        "        val_acc = test_oneshot(model,Xval,yval,sizes_val, N_way, n_val, verbose=True)\n",
        "        model.save_weights(os.path.join(model_2_path, 'weights.{}.h5'.format(i)))\n",
        "        if val_acc >= best:\n",
        "            print(\"Current best: {0}, previous best: {1}\".format(val_acc, best))\n",
        "            best = val_acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training process!\n",
            "-------------------------------------\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 200 iterations: 0.15213268200556437 mins\n",
            "Train Loss: 0.7059872150421143\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 32.0% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 32.0, previous best: -1\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 400 iterations: 0.5822814504305521 mins\n",
            "Train Loss: 0.5833199620246887\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 36.0% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 36.0, previous best: 32.0\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 600 iterations: 0.9915398955345154 mins\n",
            "Train Loss: 0.6169869303703308\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 48.4% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 48.4, previous best: 36.0\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 800 iterations: 1.4218302249908448 mins\n",
            "Train Loss: 0.5006300210952759\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 40.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 1000 iterations: 1.8278248151143393 mins\n",
            "Train Loss: 0.5681591033935547\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 46.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 1200 iterations: 2.254901921749115 mins\n",
            "Train Loss: 0.5511191487312317\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 51.2% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 51.2, previous best: 48.4\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 1400 iterations: 2.664763867855072 mins\n",
            "Train Loss: 0.35583359003067017\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 51.2% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 51.2, previous best: 51.2\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 1600 iterations: 3.099626914660136 mins\n",
            "Train Loss: 0.3379535675048828\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 54.0% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 54.0, previous best: 51.2\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 1800 iterations: 3.513097047805786 mins\n",
            "Train Loss: 0.38196009397506714\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 57.6% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 57.6, previous best: 54.0\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 2000 iterations: 3.945629533131917 mins\n",
            "Train Loss: 0.32728201150894165\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 55.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 2200 iterations: 4.354565544923147 mins\n",
            "Train Loss: 0.2989656925201416\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 61.2% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 61.2, previous best: 57.6\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 2400 iterations: 4.78056318362554 mins\n",
            "Train Loss: 0.3715244233608246\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 54.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 2600 iterations: 5.191739284992218 mins\n",
            "Train Loss: 0.278793066740036\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 60.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 2800 iterations: 5.62251318693161 mins\n",
            "Train Loss: 0.32091012597084045\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 63.2% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 63.2, previous best: 61.2\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 3000 iterations: 6.0327406287193295 mins\n",
            "Train Loss: 0.4277653694152832\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 66.4% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 66.4, previous best: 63.2\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 3200 iterations: 6.461253770192465 mins\n",
            "Train Loss: 0.23814353346824646\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 57.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 3400 iterations: 6.875580100218455 mins\n",
            "Train Loss: 0.2558363080024719\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 61.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 3600 iterations: 7.303261566162109 mins\n",
            "Train Loss: 0.2319697141647339\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 65.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 3800 iterations: 7.709732552369435 mins\n",
            "Train Loss: 0.26221880316734314\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 65.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 4000 iterations: 8.135930387179057 mins\n",
            "Train Loss: 0.24222326278686523\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 60.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 4200 iterations: 8.550122205416361 mins\n",
            "Train Loss: 0.1979971081018448\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 71.2% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 71.2, previous best: 66.4\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 4400 iterations: 8.977618873119354 mins\n",
            "Train Loss: 0.35612303018569946\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 63.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 4600 iterations: 9.383091207345327 mins\n",
            "Train Loss: 0.23416435718536377\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 60.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 4800 iterations: 9.813288505872091 mins\n",
            "Train Loss: 0.1674797683954239\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 70.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 5000 iterations: 10.224830909570057 mins\n",
            "Train Loss: 0.19676458835601807\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 67.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 5200 iterations: 10.646832092603047 mins\n",
            "Train Loss: 0.18846209347248077\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 65.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 5400 iterations: 11.057543841997783 mins\n",
            "Train Loss: 0.2818886637687683\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 63.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 5600 iterations: 11.481565582752228 mins\n",
            "Train Loss: 0.247218057513237\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 64.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 5800 iterations: 11.898460817337035 mins\n",
            "Train Loss: 0.18085792660713196\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 66.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 6000 iterations: 12.324250888824462 mins\n",
            "Train Loss: 0.2506701946258545\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 75.2% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 75.2, previous best: 71.2\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 6200 iterations: 12.738096487522125 mins\n",
            "Train Loss: 0.14078578352928162\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 68.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 6400 iterations: 13.162670318285624 mins\n",
            "Train Loss: 0.18501931428909302\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 75.6% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 75.6, previous best: 75.2\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 6600 iterations: 13.575421226024627 mins\n",
            "Train Loss: 0.3393916189670563\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 70.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 6800 iterations: 14.000598069032034 mins\n",
            "Train Loss: 0.15457503497600555\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 72.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 7000 iterations: 14.416296247641245 mins\n",
            "Train Loss: 0.2067510485649109\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 74.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 7200 iterations: 14.839155387878417 mins\n",
            "Train Loss: 0.13994571566581726\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 70.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 7400 iterations: 15.25526538292567 mins\n",
            "Train Loss: 0.1169201135635376\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 71.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 7600 iterations: 15.674684766928355 mins\n",
            "Train Loss: 0.1385353058576584\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 57.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 7800 iterations: 16.088663029670716 mins\n",
            "Train Loss: 0.19239507615566254\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 70.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 8000 iterations: 16.514120745658875 mins\n",
            "Train Loss: 0.09780538827180862\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 71.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 8200 iterations: 16.930749535560608 mins\n",
            "Train Loss: 0.18965721130371094\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 64.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 8400 iterations: 17.351231726010642 mins\n",
            "Train Loss: 0.08757729828357697\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 78.0% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 78.0, previous best: 75.6\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 8600 iterations: 17.767015977700552 mins\n",
            "Train Loss: 0.18426281213760376\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 65.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 8800 iterations: 18.195927588144936 mins\n",
            "Train Loss: 0.21673628687858582\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 68.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 9000 iterations: 18.61604716380437 mins\n",
            "Train Loss: 0.13546863198280334\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 72.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 9200 iterations: 19.04171090523402 mins\n",
            "Train Loss: 0.23662415146827698\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 62.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 9400 iterations: 19.4553662776947 mins\n",
            "Train Loss: 0.1397787481546402\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 64.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 9600 iterations: 19.881031600634255 mins\n",
            "Train Loss: 0.0989585891366005\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 69.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 9800 iterations: 20.29251116514206 mins\n",
            "Train Loss: 0.16924409568309784\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 69.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 10000 iterations: 20.715877827008566 mins\n",
            "Train Loss: 0.12873630225658417\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 64.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 10200 iterations: 21.13061213493347 mins\n",
            "Train Loss: 0.1530579924583435\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 72.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 10400 iterations: 21.553552397092183 mins\n",
            "Train Loss: 0.12203887104988098\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 77.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 10600 iterations: 21.967216606934866 mins\n",
            "Train Loss: 0.1358432173728943\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 73.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 10800 iterations: 22.395215932528178 mins\n",
            "Train Loss: 0.11508463323116302\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 75.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 11000 iterations: 22.809853772322338 mins\n",
            "Train Loss: 0.09993110597133636\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 79.6% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 79.6, previous best: 78.0\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 11200 iterations: 23.234556810061136 mins\n",
            "Train Loss: 0.10874871909618378\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 71.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 11400 iterations: 23.650474417209626 mins\n",
            "Train Loss: 0.18140541017055511\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 60.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 11600 iterations: 24.07926003932953 mins\n",
            "Train Loss: 0.13805246353149414\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 53.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 11800 iterations: 24.491719738642374 mins\n",
            "Train Loss: 0.11949855089187622\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 73.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 12000 iterations: 24.916461698214214 mins\n",
            "Train Loss: 0.11592000722885132\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 76.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 12200 iterations: 25.32971680164337 mins\n",
            "Train Loss: 0.13581600785255432\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 80.0% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 80.0, previous best: 79.6\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 12400 iterations: 25.7551655848821 mins\n",
            "Train Loss: 0.21859928965568542\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 77.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 12600 iterations: 26.167228798071545 mins\n",
            "Train Loss: 0.14681656658649445\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 68.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 12800 iterations: 26.59188648064931 mins\n",
            "Train Loss: 0.11074507981538773\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 65.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 13000 iterations: 27.00650225877762 mins\n",
            "Train Loss: 0.16253885626792908\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 82.4% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 82.4, previous best: 80.0\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 13200 iterations: 27.433167906602225 mins\n",
            "Train Loss: 0.1268644630908966\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 74.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 13400 iterations: 27.847696435451507 mins\n",
            "Train Loss: 0.11043960601091385\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 74.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 13600 iterations: 28.27674601872762 mins\n",
            "Train Loss: 0.14693425595760345\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 78.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 13800 iterations: 28.689820766448975 mins\n",
            "Train Loss: 0.12247355282306671\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 68.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 14000 iterations: 29.117775841554007 mins\n",
            "Train Loss: 0.0845833271741867\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 78.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 14200 iterations: 29.53295110066732 mins\n",
            "Train Loss: 0.09921351820230484\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 69.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 14400 iterations: 29.960248879591624 mins\n",
            "Train Loss: 0.10422319173812866\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 79.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 14600 iterations: 30.378363537788392 mins\n",
            "Train Loss: 0.14328816533088684\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 76.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 14800 iterations: 30.810047801335653 mins\n",
            "Train Loss: 0.11695195734500885\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 68.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 15000 iterations: 31.224919752279916 mins\n",
            "Train Loss: 0.09028095006942749\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 70.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 15200 iterations: 31.65116263628006 mins\n",
            "Train Loss: 0.1486632525920868\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 78.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 15400 iterations: 32.06826122601827 mins\n",
            "Train Loss: 0.1441904455423355\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 69.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 15600 iterations: 32.49438602129619 mins\n",
            "Train Loss: 0.09639304876327515\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 73.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 15800 iterations: 32.91178228457769 mins\n",
            "Train Loss: 0.10342331230640411\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 68.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 16000 iterations: 33.33673164844513 mins\n",
            "Train Loss: 0.11473511159420013\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 76.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 16200 iterations: 33.75244219700495 mins\n",
            "Train Loss: 0.11704082787036896\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 78.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 16400 iterations: 34.17735257943471 mins\n",
            "Train Loss: 0.09899038076400757\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 71.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 16600 iterations: 34.59169233640035 mins\n",
            "Train Loss: 0.12523458898067474\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 65.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 16800 iterations: 35.01956427494685 mins\n",
            "Train Loss: 0.10691113024950027\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 69.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 17000 iterations: 35.43140217463176 mins\n",
            "Train Loss: 0.10090802609920502\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 67.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 17200 iterations: 35.8548646291097 mins\n",
            "Train Loss: 0.08792389929294586\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 78.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 17400 iterations: 36.268284634749094 mins\n",
            "Train Loss: 0.09711474180221558\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 72.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 17600 iterations: 36.69436678489049 mins\n",
            "Train Loss: 0.11076454818248749\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 70.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 17800 iterations: 37.112449777126315 mins\n",
            "Train Loss: 0.10583315044641495\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 77.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 18000 iterations: 37.53708382050196 mins\n",
            "Train Loss: 0.09488525986671448\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 71.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 18200 iterations: 37.95115305980047 mins\n",
            "Train Loss: 0.09115588665008545\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 85.2% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 85.2, previous best: 82.4\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 18400 iterations: 38.37230033874512 mins\n",
            "Train Loss: 0.16184815764427185\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 82.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 18600 iterations: 38.78214625914892 mins\n",
            "Train Loss: 0.09463004767894745\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 81.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 18800 iterations: 39.20879069566727 mins\n",
            "Train Loss: 0.09700874239206314\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 70.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 19000 iterations: 39.623571407794955 mins\n",
            "Train Loss: 0.08530648052692413\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 83.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 19200 iterations: 40.04915061394374 mins\n",
            "Train Loss: 0.08914446085691452\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 77.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 19400 iterations: 40.46326402425766 mins\n",
            "Train Loss: 0.07367577403783798\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 71.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 19600 iterations: 40.8894544561704 mins\n",
            "Train Loss: 0.13150537014007568\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 78.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 19800 iterations: 41.30209529002507 mins\n",
            "Train Loss: 0.08605093508958817\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 74.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 20000 iterations: 41.73019851446152 mins\n",
            "Train Loss: 0.11973847448825836\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 81.6% 20 way one-shot learning accuracy \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4JoCm_5jbDk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "630d026e-7dd4-4310-e8d5-e7cb68e9cc03"
      },
      "source": [
        "print(\"Starting training process!\")\n",
        "print(\"-------------------------------------\")\n",
        "t_start = time.time()\n",
        "for i in range(1, n_iter+1):\n",
        "    (inputs,targets) = createPairs(X,y,sizes,batch_size)\n",
        "    targets=np.asarray(targets)\n",
        "    loss = model.train_on_batch(inputs, targets)\n",
        "    if i % evaluate_every == 0:\n",
        "        print(\"\\n ------------- \\n\")\n",
        "        print(\"Time for {0} iterations: {1} mins\".format(i, (time.time()-t_start)/60.0))\n",
        "        print(\"Train Loss: {0}\".format(loss)) \n",
        "        val_acc = test_oneshot(model,Xval,yval,sizes_val, N_way, n_val, verbose=True)\n",
        "        model.save_weights(os.path.join(model_cos_path, 'weights.{}.h5'.format(i)))\n",
        "        if val_acc >= best:\n",
        "            print(\"Current best: {0}, previous best: {1}\".format(val_acc, best))\n",
        "            best = val_acc"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training process!\n",
            "-------------------------------------\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 200 iterations: 0.1746594190597534 mins\n",
            "Train Loss: 1.099053978919983\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 32.0% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 32.0, previous best: 5.6\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 400 iterations: 0.643968419233958 mins\n",
            "Train Loss: 0.7923591136932373\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 44.4% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 44.4, previous best: 32.0\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 600 iterations: 1.0733067393302917 mins\n",
            "Train Loss: 0.6323250532150269\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 42.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 800 iterations: 1.5318235596021017 mins\n",
            "Train Loss: 0.5148969888687134\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 50.8% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 50.8, previous best: 44.4\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 1000 iterations: 1.966246787707011 mins\n",
            "Train Loss: 0.4932793974876404\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 56.8% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 56.8, previous best: 50.8\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 1200 iterations: 2.4189886411031085 mins\n",
            "Train Loss: 0.41171497106552124\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 65.6% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 65.6, previous best: 56.8\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 1400 iterations: 2.8509631951649985 mins\n",
            "Train Loss: 0.5204046964645386\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 59.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 1600 iterations: 3.307607913017273 mins\n",
            "Train Loss: 0.4015348553657532\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 79.2% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 79.2, previous best: 65.6\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 1800 iterations: 3.7368996143341064 mins\n",
            "Train Loss: 0.33342134952545166\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 78.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 2000 iterations: 4.186650820573171 mins\n",
            "Train Loss: 0.27928614616394043\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 73.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 2200 iterations: 4.616149091720581 mins\n",
            "Train Loss: 0.24110417068004608\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 71.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 2400 iterations: 5.0671799341837565 mins\n",
            "Train Loss: 0.30818530917167664\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 74.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 2600 iterations: 5.500994006792705 mins\n",
            "Train Loss: 0.25923705101013184\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 82.0% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 82.0, previous best: 79.2\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 2800 iterations: 5.958911406993866 mins\n",
            "Train Loss: 0.21257364749908447\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 73.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 3000 iterations: 6.392449295520782 mins\n",
            "Train Loss: 0.24526262283325195\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 72.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 3200 iterations: 6.8444047729174295 mins\n",
            "Train Loss: 0.3597479462623596\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 79.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 3400 iterations: 7.283158850669861 mins\n",
            "Train Loss: 0.2343580722808838\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 84.8% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 84.8, previous best: 82.0\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 3600 iterations: 7.734531438350677 mins\n",
            "Train Loss: 0.17998848855495453\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 78.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 3800 iterations: 8.172745231787363 mins\n",
            "Train Loss: 0.2834080159664154\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 84.8% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 84.8, previous best: 84.8\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 4000 iterations: 8.62520763874054 mins\n",
            "Train Loss: 0.2758420705795288\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 79.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 4200 iterations: 9.068666295210521 mins\n",
            "Train Loss: 0.17613059282302856\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 85.6% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 85.6, previous best: 84.8\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 4400 iterations: 9.52633341550827 mins\n",
            "Train Loss: 0.20929768681526184\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 86.8% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 86.8, previous best: 85.6\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 4600 iterations: 9.973100113868714 mins\n",
            "Train Loss: 0.15058918297290802\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 86.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 4800 iterations: 10.421363723278045 mins\n",
            "Train Loss: 0.20774292945861816\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 87.6% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 87.6, previous best: 86.8\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 5000 iterations: 10.858312904834747 mins\n",
            "Train Loss: 0.20485100150108337\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 78.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 5200 iterations: 11.30764109690984 mins\n",
            "Train Loss: 0.21643328666687012\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 87.6% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 87.6, previous best: 87.6\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 5400 iterations: 11.746405522028605 mins\n",
            "Train Loss: 0.11923318356275558\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 86.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 5600 iterations: 12.195228083928425 mins\n",
            "Train Loss: 0.11784292757511139\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 84.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 5800 iterations: 12.634225408236185 mins\n",
            "Train Loss: 0.11309368908405304\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 86.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 6000 iterations: 13.087394376595816 mins\n",
            "Train Loss: 0.1895127296447754\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 85.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 6200 iterations: 13.523718357086182 mins\n",
            "Train Loss: 0.15093296766281128\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 90.0% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 90.0, previous best: 87.6\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 6400 iterations: 13.973644419511158 mins\n",
            "Train Loss: 0.10634342581033707\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 89.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 6600 iterations: 14.410167586803436 mins\n",
            "Train Loss: 0.09645229578018188\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 90.4% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 90.4, previous best: 90.0\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 6800 iterations: 14.85768030087153 mins\n",
            "Train Loss: 0.09865891933441162\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 91.6% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 91.6, previous best: 90.4\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 7000 iterations: 15.293667840957642 mins\n",
            "Train Loss: 0.10510322451591492\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 85.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 7200 iterations: 15.748277533054353 mins\n",
            "Train Loss: 0.09242498874664307\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 84.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 7400 iterations: 16.18186445236206 mins\n",
            "Train Loss: 0.13275405764579773\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 84.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 7600 iterations: 16.63546304702759 mins\n",
            "Train Loss: 0.08627462387084961\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 83.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 7800 iterations: 17.06758604447047 mins\n",
            "Train Loss: 0.18016386032104492\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 89.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 8000 iterations: 17.523516643047333 mins\n",
            "Train Loss: 0.33448415994644165\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 86.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 8200 iterations: 17.96790484984716 mins\n",
            "Train Loss: 0.21045321226119995\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 82.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 8400 iterations: 18.426459741592407 mins\n",
            "Train Loss: 0.27891379594802856\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 83.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 8600 iterations: 18.868672899405162 mins\n",
            "Train Loss: 0.08785413950681686\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 84.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 8800 iterations: 19.327458918094635 mins\n",
            "Train Loss: 0.07562282681465149\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 85.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 9000 iterations: 19.75818886756897 mins\n",
            "Train Loss: 0.08521339297294617\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 85.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 9200 iterations: 20.207601435979207 mins\n",
            "Train Loss: 0.08059345185756683\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 88.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 9400 iterations: 20.642009635766346 mins\n",
            "Train Loss: 0.08334529399871826\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 86.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 9600 iterations: 21.101701617240906 mins\n",
            "Train Loss: 0.08148141205310822\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 91.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 9800 iterations: 21.541348123550414 mins\n",
            "Train Loss: 0.06975436210632324\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 88.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 10000 iterations: 21.99855525890986 mins\n",
            "Train Loss: 0.07574895024299622\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 92.8% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 92.8, previous best: 91.6\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 10200 iterations: 22.433670337994894 mins\n",
            "Train Loss: 0.07996026426553726\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 88.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 10400 iterations: 22.889137558142345 mins\n",
            "Train Loss: 0.2020760178565979\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 85.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 10600 iterations: 23.326107434431712 mins\n",
            "Train Loss: 0.08471935987472534\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 88.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 10800 iterations: 23.784267342090608 mins\n",
            "Train Loss: 0.06337989866733551\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 91.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 11000 iterations: 24.222787523269652 mins\n",
            "Train Loss: 0.0836862325668335\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 94.4% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 94.4, previous best: 92.8\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 11200 iterations: 24.677334952354432 mins\n",
            "Train Loss: 0.23277388513088226\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 89.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 11400 iterations: 25.120954068501792 mins\n",
            "Train Loss: 0.14467717707157135\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 93.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 11600 iterations: 25.572389713923137 mins\n",
            "Train Loss: 0.0611049048602581\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 88.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 11800 iterations: 26.010975710550944 mins\n",
            "Train Loss: 0.0673385038971901\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 89.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 12000 iterations: 26.461334311962126 mins\n",
            "Train Loss: 0.08342976868152618\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 91.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 12200 iterations: 26.89828411738078 mins\n",
            "Train Loss: 0.06684495508670807\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 93.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 12400 iterations: 27.349605182806652 mins\n",
            "Train Loss: 0.06394453346729279\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 89.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 12600 iterations: 27.78601746161779 mins\n",
            "Train Loss: 0.10656052827835083\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 88.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 12800 iterations: 28.236573080221813 mins\n",
            "Train Loss: 0.07270816713571548\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 91.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 13000 iterations: 28.679081253210704 mins\n",
            "Train Loss: 0.10347452759742737\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 90.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 13200 iterations: 29.12898324330648 mins\n",
            "Train Loss: 0.12421762198209763\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 90.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 13400 iterations: 29.56673533519109 mins\n",
            "Train Loss: 0.05893758684396744\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 86.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 13600 iterations: 30.016030434767405 mins\n",
            "Train Loss: 0.05668092519044876\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 93.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 13800 iterations: 30.471254968643187 mins\n",
            "Train Loss: 0.0590299591422081\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 90.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 14000 iterations: 30.936687342325847 mins\n",
            "Train Loss: 0.054167989641427994\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 88.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 14200 iterations: 31.377377466360727 mins\n",
            "Train Loss: 0.05277037248015404\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 90.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 14400 iterations: 31.83199320236842 mins\n",
            "Train Loss: 0.1381649374961853\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 88.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 14600 iterations: 32.262255346775056 mins\n",
            "Train Loss: 0.05275460705161095\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 94.4% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 94.4, previous best: 94.4\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 14800 iterations: 32.716078253587085 mins\n",
            "Train Loss: 0.05410751700401306\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 90.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 15000 iterations: 33.145687305927275 mins\n",
            "Train Loss: 0.050888169556856155\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 90.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 15200 iterations: 33.60156026681264 mins\n",
            "Train Loss: 0.11807184666395187\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 88.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 15400 iterations: 34.04325583775838 mins\n",
            "Train Loss: 0.05108882486820221\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 91.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 15600 iterations: 34.51044586102168 mins\n",
            "Train Loss: 0.058369193226099014\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 93.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 15800 iterations: 34.945864482720694 mins\n",
            "Train Loss: 0.048506248742341995\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 87.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 16000 iterations: 35.40264925956726 mins\n",
            "Train Loss: 0.05213157832622528\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 93.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 16200 iterations: 35.84075493415197 mins\n",
            "Train Loss: 0.05249248817563057\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 90.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 16400 iterations: 36.3015625278155 mins\n",
            "Train Loss: 0.049784623086452484\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 95.6% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 95.6, previous best: 94.4\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 16600 iterations: 36.7371985634168 mins\n",
            "Train Loss: 0.07430020719766617\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 95.6% 20 way one-shot learning accuracy \n",
            "\n",
            "Current best: 95.6, previous best: 95.6\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 16800 iterations: 37.19363648096721 mins\n",
            "Train Loss: 0.06383240967988968\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 92.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 17000 iterations: 37.63110029697418 mins\n",
            "Train Loss: 0.0479348748922348\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 92.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 17200 iterations: 38.08209822972616 mins\n",
            "Train Loss: 0.04714309796690941\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 92.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 17400 iterations: 38.51546502113342 mins\n",
            "Train Loss: 0.06913779675960541\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 92.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 17600 iterations: 38.966032127539314 mins\n",
            "Train Loss: 0.045809775590896606\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 89.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 17800 iterations: 39.40655783812205 mins\n",
            "Train Loss: 0.08614779263734818\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 90.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 18000 iterations: 39.86267168919245 mins\n",
            "Train Loss: 0.04605609551072121\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 92.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 18200 iterations: 40.30662132104238 mins\n",
            "Train Loss: 0.04606236517429352\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 94.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 18400 iterations: 40.756139957904814 mins\n",
            "Train Loss: 0.08583875000476837\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 91.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 18600 iterations: 41.190426127115884 mins\n",
            "Train Loss: 0.05568676069378853\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 90.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 18800 iterations: 41.629124609629315 mins\n",
            "Train Loss: 0.04414251446723938\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 90.4% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 19000 iterations: 42.06066465775172 mins\n",
            "Train Loss: 0.04618705064058304\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 90.8% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 19200 iterations: 42.50009829203288 mins\n",
            "Train Loss: 0.04987756535410881\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 91.6% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 19400 iterations: 42.93675682942072 mins\n",
            "Train Loss: 0.044829171150922775\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 91.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 19600 iterations: 43.39305727481842 mins\n",
            "Train Loss: 0.06317868828773499\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 92.0% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 19800 iterations: 43.8301841934522 mins\n",
            "Train Loss: 0.06860257685184479\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 95.2% 20 way one-shot learning accuracy \n",
            "\n",
            "\n",
            " ------------- \n",
            "\n",
            "Time for 20000 iterations: 44.27989245653153 mins\n",
            "Train Loss: 0.04542369395494461\n",
            "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
            "\n",
            "Got an average of 88.0% 20 way one-shot learning accuracy \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcTZ3HHJjbDp"
      },
      "source": [
        "## loading model from weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yt8pqshBxKcQ"
      },
      "source": [
        "# model_path="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XSzGDRhjbDr"
      },
      "source": [
        "model.load_weights(model_cos_path+'weights.20000.h5')"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFJVIYiajbDs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDQXUEmFjbDt"
      },
      "source": [
        "def calc_accuracy(N,Xval,yval,anchor_img,anchor_label,model):\n",
        "    count_first=0\n",
        "    count_first3=0\n",
        "    for i in range(N):\n",
        "        ind=random.choice(range(yval.shape[0]))\n",
        "        predicted,anchor_imgs,targets=whichGlyph(model,Xval[ind],anchor_img,anchor_label)\n",
        "        sort_index = np.argsort(np.asarray(predicted).reshape(len(predicted),))\n",
        "        if targets[sort_index[-1]] == yval[ind][0]:\n",
        "            count_first+=1\n",
        "        if yval[ind][0] in targets[sort_index[127:]]:\n",
        "            count_first3+=1\n",
        "    accuracy_first=count_first/N\n",
        "    accuracy_first3=count_first3/N\n",
        "    \n",
        "    return accuracy_first, accuracy_first3"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmLQ6lVYjbDt",
        "outputId": "5a756d04-6f66-4f17-a33d-31ff18567676"
      },
      "source": [
        "t_start = time.time()\n",
        "acc1,acc3=calc_accuracy(1500,Xval,yval,anchor_img,anchor_label,model)\n",
        "print(f'testing:\\nfound first accuracy = {acc1} , first 3 accuracy = {acc3} ')\n",
        "print(\"accuracy fn took {0} mins\".format((time.time()-t_start)/60.0))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "testing:\n",
            "found first accuracy = 0.6213333333333333 , first 3 accuracy = 0.9673333333333334 \n",
            "accuracy fn took 2.7014573335647585 mins\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdtRB3qAjbDw",
        "outputId": "622b1c1c-0e99-4905-f832-f337cfa626fa"
      },
      "source": [
        "t_start = time.time()\n",
        "acc1,acc3=calc_accuracy(250,X,y,anchor_img,anchor_label,model)\n",
        "print(f'training:\\nfound first accuracy = {acc1} , first 3 accuracy = {acc3} ')\n",
        "print(\"accuracy fn took {0} mins\".format((time.time()-t_start)/60.0))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training:\n",
            "found first accuracy = 0.8 , first 3 accuracy = 1.0 \n",
            "accuracy fn took 0.4688496947288513 mins\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poAicP_njbDx"
      },
      "source": [
        "# t_start = time.time()\n",
        "# print(calc_accuracy(1,Xval,yval,anchor_img,anchor_label,model))\n",
        "# print(\"accuracy fn took {0} sec\".format((time.time()-t_start)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beW1a29IjbDz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrLEK0fejbDz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZplSFXGjbD3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}