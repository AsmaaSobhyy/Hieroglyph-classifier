{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "S9EUfmmBja-_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import skimage.io as io\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "#for colab\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "from tensorflow.keras.layers import Layer\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "# from tensorflow.keras.preprocessing import image\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Dense, GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXwbeZYYjbAJ"
   },
   "source": [
    "## Loading data\n",
    "a function that loads the images locations and labels.<br>\n",
    "input: the data path.<br>\n",
    "output:<br>\n",
    "&emsp;&emsp;dataHiero -> a dataframe with index= location of images and label= their labels <br>\n",
    "&emsp;&emsp;img_groups -> a dictionary in the shape of { \"label\" : [array of locations of images labeled with this label] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "noWbwGlLjbAY"
   },
   "outputs": [],
   "source": [
    "path=\"../GlyphDataset/Dataset/Manual/Preprocessed/\"\n",
    "\n",
    "def loadData(folderPictures=path):\n",
    "    \n",
    "    folders=next(os.walk(folderPictures))[1]\n",
    "    img_groups = {}\n",
    "    img_list={}\n",
    "\n",
    "    for folder in folders:\n",
    "        for img_file in os.listdir(folderPictures+folder):\n",
    "            name, \n",
    "            label = img_file.strip('.png').split(\"_\")\n",
    "            \n",
    "            \n",
    "            # One image per class\n",
    "\n",
    "            #if label not in img_groups.keys():\n",
    "            #    img_groups[label] = [folder + \"_\" + name]\n",
    "\n",
    "\n",
    "            # Multiple images per class\n",
    "\n",
    "            if label in img_groups.keys():\n",
    "                img_groups[label].append(folder+\"_\"+name)\n",
    "            else:\n",
    "                img_groups[label] = [folder+\"_\"+name]\n",
    "\n",
    "            img_list[folder+\"_\"+name]=[label]\n",
    "\n",
    "\n",
    "    # Remove class with only one hieroglyph\n",
    "\n",
    "\n",
    "    for k,v in list(img_groups.items()):\n",
    "        if len(v)==1: del img_groups[k]\n",
    "\n",
    "    # Extract only N hieroglyph classes randomly\n",
    "\n",
    "    nclass = len(img_groups.keys())\n",
    "\n",
    "    list_of_class = random.sample(list(img_groups.keys()), nclass)\n",
    "#     print(list_of_class)\n",
    "\n",
    "    short_dico = {x: img_groups[x] for x in list_of_class if x in img_groups}\n",
    "\n",
    "    dataHiero=pd.DataFrame.from_dict(img_list,orient='index')\n",
    "    dataHiero.columns = [\"label\"]\n",
    "    dataHiero = dataHiero[dataHiero.label != 'UNKNOWN']\n",
    "\n",
    "    dataHiero = dataHiero.loc[dataHiero['label'].isin(short_dico)]\n",
    "\n",
    "\n",
    "    dataHiero.reset_index(level=0, inplace=True)\n",
    "\n",
    "    return dataHiero,img_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CaZM8DxjbAp"
   },
   "source": [
    "a function that takes the image groups and load those images<br>\n",
    "input: img_proups dictionary<br>\n",
    "output:<br>\n",
    "&emsp;&emsp;X -> np array of the images<br>\n",
    "&emsp;&emsp;y -> np array of labels<br>\n",
    "&emsp;&emsp;glyph_sizes -> a dictionary in the form of {'label' : (starting index, ending index in X and y)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xO2UaDgTjbAu"
   },
   "outputs": [],
   "source": [
    "def read_images(img_groups,path):\n",
    "    X=[]\n",
    "    y=[]\n",
    "    glyph_sizes={}\n",
    "    low=0\n",
    "    for glyph in img_groups:\n",
    "        category_images=[]\n",
    "        high=low\n",
    "        for img_path in img_groups[glyph] :\n",
    "            folder,name = img_path.split('_')\n",
    "            image = io.imread(path+folder+'/'+name+'_'+glyph+'.png')\n",
    "            X.append(image)\n",
    "            y.append(glyph)\n",
    "            high+=1\n",
    "#         X.append(np.array(category_images))\n",
    "        glyph_sizes[glyph]=(low,high-1)\n",
    "        low=high\n",
    "        \n",
    "    return np.array(X),np.array(y).reshape((-1,1)),glyph_sizes\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "id": "IYwf13e4jbA3",
    "outputId": "40164a56-2bf4-4e9f-9e17-823f4146688c"
   },
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4ff87f925fef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataHiero\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_groups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolderPictures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdataHiero\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# img_groups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-62ed25224a45>\u001b[0m in \u001b[0;36mloadData\u001b[0;34m(folderPictures)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolderPictures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mfolders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolderPictures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mimg_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mimg_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataHiero,img_groups=loadData(folderPictures=path)\n",
    "dataHiero.head()\n",
    "# img_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-j6RWKkrjbA6"
   },
   "outputs": [],
   "source": [
    "X,y,sizes=read_images(img_groups,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kNTAe8oQjbBG",
    "outputId": "a030e446-b195-4481-b03e-2b201b17045f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2921, 1)\n",
      "(2921, 75, 50)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)\n",
    "print(y.shape)\n",
    "print(X.shape)\n",
    "sizes['D21'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjQdogROjbBI"
   },
   "source": [
    "saving the images into a pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Br2-Xm7bjbBb"
   },
   "outputs": [],
   "source": [
    "#train val split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPS8xCJPjbBh"
   },
   "outputs": [],
   "source": [
    "def get_sizes(X,Y):\n",
    "    sizes={}\n",
    "    for i ,(x,y) in enumerate(zip(X,Y)):\n",
    "#         print(i,x,y)\n",
    "        if y[0] in sizes:\n",
    "            sizes[y[0]].append(i)\n",
    "        else:\n",
    "            sizes[y[0]]=[i]\n",
    "    return sizes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sG1c9UUHjbBi"
   },
   "outputs": [],
   "source": [
    "sizes=get_sizes(X_train,y_train)\n",
    "X=X_train\n",
    "y=y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbrU-fAkjbBn"
   },
   "outputs": [],
   "source": [
    "#saving data as pickle\n",
    "with open(\"train.pickle\", \"wb\") as f:\n",
    "    pickle.dump((X,y,sizes),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4szlW2XjbBo"
   },
   "outputs": [],
   "source": [
    "sizes_val=get_sizes(X_test,y_test)\n",
    "Xval=X_test\n",
    "yval=y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NkrX6IOujbBp"
   },
   "outputs": [],
   "source": [
    "#saving data as pickle\n",
    "with open(\"test.pickle\", \"wb\") as f:\n",
    "    pickle.dump((Xval,yval,sizes_val),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cUfovN9vjbBw"
   },
   "outputs": [],
   "source": [
    "# #saving data as pickle\n",
    "# with open(\"test.pickle\", \"wb\") as f:\n",
    "#     pickle.dump((Xval,yval,sizes_val),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtd7kUY6jbBx"
   },
   "source": [
    "## reading the training tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XVz6vsHZ4SeJ"
   },
   "outputs": [],
   "source": [
    "#colab\n",
    "# data_path= '/content/drive/MyDrive/hiero_cv/'\n",
    "#local\n",
    "data_path='./'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k19Gi0fR4BU9",
    "outputId": "1d477b0f-8711-4576-acc3-0ffa4e7307f7"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9ZPC6wKrjbB4"
   },
   "outputs": [],
   "source": [
    "with open(data_path+\"train.pickle\", \"rb\") as f:\n",
    "    (X,y,sizes) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lOstoN_vjbB7"
   },
   "outputs": [],
   "source": [
    "with open(data_path+\"test.pickle\", \"rb\") as f:\n",
    "    (Xval,yval,sizes_val) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EmdcIQiwpDLV"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For later : weight initialization approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "XOB0W_VqjbB8"
   },
   "outputs": [],
   "source": [
    "\n",
    "# def initialize_weights(shape, name=None):\n",
    "#     \"\"\"\n",
    "#         The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "#         suggests to initialize CNN layer weights with mean as 0.0 and standard deviation of 0.01\n",
    "#     \"\"\"\n",
    "#     return np.random.normal(loc = 0.0, scale = 1e-2, size = shape)\n",
    "\n",
    "# def initialize_bias(shape, name=None):\n",
    "#     \"\"\"\n",
    "#         The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "#         suggests to initialize CNN layer bias with mean as 0.5 and standard deviation of 0.01\n",
    "#     \"\"\"\n",
    "#     return np.random.normal(loc = 0.5, scale = 1e-2, size = shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xwz8vTAmjbCB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xJN3eoVjbCQ"
   },
   "source": [
    "### model 2 : lower accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G63fmh8-jbCR"
   },
   "outputs": [],
   "source": [
    "# def get_siamese_model_2(input_shape):\n",
    "#     \"\"\"\n",
    "#         Model architecture based on the one provided in: http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Define the tensors for the two input images\n",
    "#     left_input = Input(input_shape)\n",
    "#     right_input = Input(input_shape)\n",
    "    \n",
    "#     # Convolutional Neural Network\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv2D(64, (3,3),strides=(2, 2), activation='relu', input_shape=input_shape, kernel_regularizer=l2(2e-4)))\n",
    "#     model.add(MaxPooling2D((2, 2), strides=2))\n",
    "#     model.add(Conv2D(64, (3,3), activation='relu', kernel_regularizer=l2(2e-4)))\n",
    "#     model.add(MaxPooling2D((2, 2), strides=2))\n",
    "#     model.add(Conv2D(128, (3,3), activation='relu', kernel_regularizer=l2(2e-4)))\n",
    "# #     model.add(MaxPooling2D())\n",
    "# #     model.add(Conv2D(256, (4,4), activation='relu',  kernel_regularizer=l2(2e-4)))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(4096, activation='sigmoid',\n",
    "#                    kernel_regularizer=l2(1e-3)))\n",
    "    \n",
    "#     # Generate the encodings (feature vectors) for the two images\n",
    "#     encoded_l = model(left_input)\n",
    "#     encoded_r = model(right_input)\n",
    "    \n",
    "#     # Add a customized layer to compute the absolute difference between the encodings\n",
    "#     L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "#     L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    \n",
    "#     # Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "#     prediction = Dense(1,activation='sigmoid')(L1_distance)\n",
    "    \n",
    "#     # Connect the inputs with the outputs\n",
    "#     siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    \n",
    "#     # return the model\n",
    "#     return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pLzzLePsjbCS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model_2 = get_siamese_model_2((75, 50, 1))\n",
    "# model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kk2Tsft9jbCT"
   },
   "outputs": [],
   "source": [
    "# optimizer = Adam(lr = 0.001)\n",
    "# model_2.compile(loss=\"binary_crossentropy\",optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFcNXLnxjbCT"
   },
   "source": [
    "### Later : try transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zodoNhCYjbCU"
   },
   "outputs": [],
   "source": [
    "# def transfer_model(input_shape):\n",
    "#     \"\"\"\n",
    "#         Model architecture based on the one provided in: http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Define the tensors for the two input images\n",
    "#     left_input = Input(input_shape)\n",
    "#     right_input = Input(input_shape)\n",
    "    \n",
    "#     #Import inception model for transfer learning without output layers\n",
    "#     base_model = InceptionV3(weights='imagenet', include_top=False, input_shape = input_shape)\n",
    "    \n",
    "    \n",
    "#     x = base_model.output\n",
    "#     x = GlobalAveragePooling2D()(x)\n",
    "#     # let's add a fully-connected layer\n",
    "#     x = Dense(1024, activation='relu')(x)\n",
    "#     # and a logistic layer -- let's say we have 200 classes\n",
    "#     model= Dense(200, activation='softmax')(x)\n",
    "    \n",
    "#     # this is the model we will train\n",
    "# #     model = Model(inputs=base_model.input, outputs=predictions)\n",
    "        \n",
    "    \n",
    "#     # Generate the encodings (feature vectors) for the two images\n",
    "#     encoded_l = model(left_input)\n",
    "#     encoded_r = model(right_input)\n",
    "    \n",
    "#     # Add a customized layer to compute the absolute difference between the encodings\n",
    "#     L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "#     L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    \n",
    "#     # Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "#     prediction = Dense(1,activation='sigmoid')(L1_distance)\n",
    "    \n",
    "#     # Connect the inputs with the outputs\n",
    "#     model = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    \n",
    "#     for layer in model.layers[:249]:\n",
    "#        layer.trainable = False\n",
    "#     for layer in model.layers[249:]:\n",
    "#        layer.trainable = True\n",
    "    \n",
    "#     # return the model\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-fIRWtajbCV"
   },
   "outputs": [],
   "source": [
    "# inception_model = transfer_model((75, 50, 1))\n",
    "# inception_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zEfmyM_KjbCW"
   },
   "outputs": [],
   "source": [
    "# inception_model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "uRlqnliu5So9"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "JpR7qVOCjbCX"
   },
   "outputs": [],
   "source": [
    "def get_siamese_model(input_shape):\n",
    "    \"\"\"\n",
    "        Model architecture based on the one provided in: http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the tensors for the two input images\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "    \n",
    "    # Convolutional Neural Network\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (3,3), activation='relu', input_shape=input_shape, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(64, (3,3), activation='relu', kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (4,4), activation='relu', kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(256, (4,4), activation='relu',  kernel_regularizer=l2(2e-4)))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(4096, activation='relu',\n",
    "                   kernel_regularizer=l2(1e-3)))\n",
    "    \n",
    "    # Generate the encodings (feature vectors) for the two images\n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "    \n",
    "    # Add a customized layer to compute the absolute difference between the encodings\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    \n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    L1_distance = Dense(512,activation='relu',kernel_regularizer=l2(1e-3))(L1_distance)\n",
    "    L1_distance = Dense(256,activation='relu',kernel_regularizer=l2(1e-3))(L1_distance)\n",
    "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "\n",
    "    prediction = Dense(1,activation='sigmoid')(L1_distance)\n",
    "\n",
    "    #cosine similarity (not learning anything)\n",
    "    # def cosine_distance(vests):\n",
    "    #   x, y = vests\n",
    "    #   x = K.l2_normalize(x, axis=-1)\n",
    "    #   y = K.l2_normalize(y, axis=-1)\n",
    "    #   return -K.mean(x * y, axis=-1, keepdims=True)\n",
    "    \n",
    "    \n",
    "    # prediction = Lambda(cosine_distance, output_shape=1)([encoded_l, encoded_r])\n",
    "    # Connect the inputs with the outputs\n",
    "    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    \n",
    "    # return the model\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4MtvWYCyjbCb",
    "outputId": "0d839893-4eaf-4adf-9e2a-11790c4c8914"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 75, 50, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 75, 50, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_4 (Sequential)       (None, 4096)         4891712     input_7[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 4096)         0           sequential_4[1][0]               \n",
      "                                                                 sequential_4[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 512)          2097664     lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 256)          131328      dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 1)            257         dense_15[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 7,120,961\n",
      "Trainable params: 7,120,961\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_siamese_model((75, 50, 1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FpDd_9CWjbCd",
    "outputId": "4d10cd4f-49e9-41b7-e7a6-6eafa8b9be4e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asmaa/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(lr = 0.0001)\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNSpnQaljbCe"
   },
   "source": [
    "a function that create pairs of images with y= 1 if they are similar and 0 if they are different. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUBHJas8jbCf"
   },
   "source": [
    "## a function to predict which glyph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Kua3lTs3jbCf"
   },
   "outputs": [],
   "source": [
    "def create_glyphlist(X,sizes):\n",
    "    images=[]\n",
    "    labels=[]\n",
    "    _,w,h=X.shape\n",
    "    for glyph in sizes:\n",
    "        index=sizes[glyph][0]\n",
    "        images.append(X[index].reshape( w , h, 1))\n",
    "        labels.append(glyph)\n",
    "    return np.asarray(images), np.asarray(labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "DTWwXWyBjbCn"
   },
   "outputs": [],
   "source": [
    "anchor_img, anchor_label=create_glyphlist(X,sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UV5F8TRwjbCo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "cj5krLHkjbCo"
   },
   "outputs": [],
   "source": [
    "def whichGlyph_pair(image,anchor_img,anchor_label):\n",
    "    N,w,h,_=anchor_img.shape\n",
    "#     pairs=[np.zeros((N, w, h,1)) for i in range(2)]\n",
    "    \n",
    "    test_image= np.asarray([image]*N).reshape(N, w, h,1)\n",
    "    \n",
    "    anchor_label, test_image, anchor_img = shuffle(anchor_label, test_image, anchor_img)\n",
    "#     pairs = [test_image,anchor_img]\n",
    "    \n",
    "    return test_image, anchor_img, anchor_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "e6a5mx_MjbCq"
   },
   "outputs": [],
   "source": [
    "def whichGlyph(model,image,anchor_img,anchor_label):\n",
    "    test_image,anchor_img,targets = whichGlyph_pair(image,anchor_img,anchor_label)\n",
    "    probs = model.predict([test_image,anchor_img])\n",
    "    return probs,anchor_img,targets\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xCa5bCWjbC6"
   },
   "source": [
    "## creating pairs of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "a7t5J2GCjbC7"
   },
   "outputs": [],
   "source": [
    "def createPairs(X,y,sizes,batch_size):\n",
    "    ##create a batch with half it's size are similar glyphs and the other half are different.\n",
    "    n=0\n",
    "    i=0\n",
    "    \n",
    "    label=[]\n",
    "    _,w,h=X.shape\n",
    "    # initialize 2 empty arrays for the input image batch\n",
    "#     pairs=[np.zeros((batch_size, w, h,1)) for i in range(2)]\n",
    "    input1=np.zeros((batch_size, w, h,1))\n",
    "    input2=np.zeros((batch_size, w, h,1))\n",
    "    \n",
    "    while n < batch_size:\n",
    "        random_key1=random.choice(list(sizes))\n",
    "#         low=sizes[random_key1][0]\n",
    "#         high=sizes[random_key1][1]\n",
    "        index1, index3 = np.random.choice(sizes[random_key1], size=2)\n",
    "        index2 = np.random.choice(sizes[random_key1])\n",
    "        random_key2=random.choice(list(sizes))\n",
    "        \n",
    "        while random_key2 == random_key1:\n",
    "            random_key2=random.choice(list(sizes))\n",
    "            \n",
    "#         low=sizes[random_key2][0]\n",
    "#         high=sizes[random_key2][1]\n",
    "        index4=np.random.choice(sizes[random_key2])\n",
    "        n += 2\n",
    "        # appending images 1 and 3 into input1 and input2 corresponding to y=1 \n",
    "        #and images 2 and 4 corresponding to y=0\n",
    "    \n",
    "        input1[i,:,:,:] = X[index1].reshape( w , h, 1)\n",
    "        input1[i+1,:,:,:] = X[index2].reshape(w, h, 1)\n",
    "        input2[i,:,:,:] = X[index3].reshape(w, h, 1)\n",
    "        input2[i+1,:,:,:] = X[index4].reshape(w, h, 1)\n",
    "        i += 2\n",
    "#         input1+=[X[index1],X[index2]]\n",
    "#         input2+=[X[index3],X[index4]]\n",
    "        label+=[1,0]\n",
    "        \n",
    "#         print(index1,index2,index3,index4)\n",
    "#         print(y[index1],y[index2],y[index3],y[index4])\n",
    "#         print(random_key1,random_key2)\n",
    "    input1,input2,label = shuffle(input1,input2,label)\n",
    "    pairs=[input1,input2]\n",
    "    \n",
    "    return pairs,label\n",
    "pairs,label=createPairs(X,y,sizes,32)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "BXoehBUTjbC9"
   },
   "outputs": [],
   "source": [
    "# pairs[1].shape\n",
    "# label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alqGHaxLjbC-"
   },
   "source": [
    "a fn that creates a N-way one shot learning task where it create pairs with the wanted image and N-1 different ones and 1 similar one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "eVWV2CnbjbC-"
   },
   "outputs": [],
   "source": [
    "def make_oneshot_task(X,y,sizes,N):\n",
    "    _,w,h=X.shape\n",
    "    pairs=[np.zeros((N, w, h,1)) for i in range(2)]\n",
    "    \n",
    "    true_key=random.choice(list(sizes))\n",
    "#     low=sizes[true_key][0]\n",
    "#     high=sizes[true_key][1]\n",
    "    \n",
    "    index=np.random.choice(sizes[true_key])\n",
    "    index2=np.random.choice(sizes[true_key])\n",
    "    \n",
    "    test_image= np.asarray([X[index]]*N).reshape(N, w, h,1)\n",
    "#     print(X.shape)\n",
    "    X_diff= np.delete(X,sizes[true_key],axis=0)\n",
    "#     print(X_diff.shape)\n",
    "    indices= np.random.choice(range(0, len(X_diff)), size = N-1)\n",
    "    \n",
    "    support_set=X_diff[indices,:,:]\n",
    "    ##!!! adding the similar image to the start of the array is not working!!!!!!!!!!!!!!!!!!!!!\n",
    "    \n",
    "#     print(support_set.shape)\n",
    "#     support_set = [ X[index2] ] + support_set\n",
    "    support_set=np.insert(support_set,0,X[index2],axis=0)\n",
    "#     print(support_set.shape)\n",
    "    targets = np.zeros((N,))\n",
    "    targets[0] = 1\n",
    "    support_set=support_set.reshape(N,w,h,1)\n",
    "    targets, test_image, support_set = shuffle(targets, test_image, support_set)\n",
    "    pairs = [test_image,support_set]\n",
    "\n",
    "    return pairs, targets\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "27BD6Rg9jbDO",
    "outputId": "829520be-1200-42c2-c4eb-5ba507437ae0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 75, 50, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp,tt=make_oneshot_task(Xval,yval,sizes_val,20) \n",
    "pp[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H6lenk9bjbDP",
    "outputId": "582d0151-dd6a-43f8-ac93-bf6c9bc57fed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 75, 50, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plt.imshow(pairs[0][0])\n",
    "pairs[1].shape\n",
    "# len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "wr6bSOuBjbDZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "r8POHa_JjbDZ"
   },
   "outputs": [],
   "source": [
    "def test_oneshot(model,X,y,sizes, N, k, s = \"val\", verbose = 0):\n",
    "    \"\"\"Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks\"\"\"\n",
    "    n_correct = 0\n",
    "    if verbose:\n",
    "        print(\"Evaluating model on {} random {} way one-shot learning tasks ... \\n\".format(k,N))\n",
    "    for i in range(k):\n",
    "        inputs, targets = make_oneshot_task(X,y,sizes,N)\n",
    "        probs = model.predict(inputs)\n",
    "        if np.argmax(probs) == np.argmax(targets):\n",
    "            n_correct+=1\n",
    "    percent_correct = (100.0 * n_correct / k)\n",
    "    if verbose:\n",
    "        print(\"Got an average of {}% {} way one-shot learning accuracy \\n\".format(percent_correct,N))\n",
    "    return percent_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "RbUqy4FNjbDf"
   },
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "evaluate_every = 200 # interval for evaluating on one-shot tasks\n",
    "batch_size = 128 #32\n",
    "n_iter = 20000 # No. of training iterations 20000\n",
    "N_way = 20 # how many classes for testing one-shot tasks\n",
    "n_val = 250 # how many one-shot tasks to validate on\n",
    "best = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "fJXH6WomjbDg"
   },
   "outputs": [],
   "source": [
    "model_path = './weights/'\n",
    "# model_2_path= '/content/drive/MyDrive/hiero_cv/weights/'\n",
    "# model_cos_path= '/content/drive/MyDrive/hiero_cv/weights_cos/'\n",
    "# model_regul='/content/drive/MyDrive/hiero_cv/regul_weights/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gUJcfqJLjbDh",
    "outputId": "828df75e-643c-4a76-9864-100a3d62f41c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2921, 75, 50)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "xwLw68YljbDj"
   },
   "outputs": [],
   "source": [
    "# print(\"Starting training process!\")\n",
    "# print(\"-------------------------------------\")\n",
    "# t_start = time.time()\n",
    "# for i in range(1, n_iter+1):\n",
    "#     (inputs,targets) = createPairs(X,y,sizes,batch_size)\n",
    "#     targets=np.asarray(targets)\n",
    "#     loss = model.train_on_batch(inputs, targets)\n",
    "#     if i % evaluate_every == 0:\n",
    "#         print(\"\\n ------------- \\n\")\n",
    "#         print(\"Time for {0} iterations: {1} mins\".format(i, (time.time()-t_start)/60.0))\n",
    "#         print(\"Train Loss: {0}\".format(loss)) \n",
    "#         val_acc = test_oneshot(model,Xval,yval,sizes_val, N_way, n_val, verbose=True)\n",
    "#         model.save_weights(os.path.join(model_regul, 'weights.{}.h5'.format(i)))\n",
    "#         if val_acc >= best:\n",
    "#             print(\"Current best: {0}, previous best: {1}\".format(val_acc, best))\n",
    "#             best = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A4JoCm_5jbDk",
    "outputId": "af521ab9-ddd2-48b3-977d-8205edb5d555"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training process!\n",
      "-------------------------------------\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 200 iterations: 0.4620641112327576 mins\n",
      "Train Loss: 1.9128260612487793\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 26.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 26.4, previous best: -1\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 400 iterations: 1.1809937556584675 mins\n",
      "Train Loss: 1.3017417192459106\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 52.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 52.8, previous best: 26.4\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 600 iterations: 1.8878071069717408 mins\n",
      "Train Loss: 1.0451736450195312\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 66.0, previous best: 52.8\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 800 iterations: 2.5838430166244506 mins\n",
      "Train Loss: 0.9491598010063171\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 77.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 77.2, previous best: 66.0\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 1000 iterations: 3.275458884239197 mins\n",
      "Train Loss: 0.8233011960983276\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 83.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 83.6, previous best: 77.2\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 1200 iterations: 3.9572367668151855 mins\n",
      "Train Loss: 0.7067266702651978\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 1400 iterations: 4.66034593184789 mins\n",
      "Train Loss: 0.6853015422821045\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 81.6% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 1600 iterations: 5.356338814894358 mins\n",
      "Train Loss: 0.6576581001281738\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 1800 iterations: 6.063238215446472 mins\n",
      "Train Loss: 0.5144249200820923\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 2000 iterations: 6.762298234303793 mins\n",
      "Train Loss: 0.47299954295158386\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 83.2% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 2200 iterations: 7.448784021536509 mins\n",
      "Train Loss: 0.4603181481361389\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 83.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 83.6, previous best: 83.6\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 2400 iterations: 8.13301347096761 mins\n",
      "Train Loss: 0.4173760414123535\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 82.8% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 2600 iterations: 8.819258042176564 mins\n",
      "Train Loss: 0.37863078713417053\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 82.4% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 2800 iterations: 9.501268271605174 mins\n",
      "Train Loss: 0.41397738456726074\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 85.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 85.2, previous best: 83.6\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 3000 iterations: 10.173567938804627 mins\n",
      "Train Loss: 0.30643901228904724\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 86.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 86.0, previous best: 85.2\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 3200 iterations: 10.875198765595753 mins\n",
      "Train Loss: 0.312138170003891\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 77.6% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 3400 iterations: 11.582143207391104 mins\n",
      "Train Loss: 0.2615923583507538\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 83.2% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 3600 iterations: 12.281855014959971 mins\n",
      "Train Loss: 0.25826403498649597\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 84.8% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 3800 iterations: 12.96403740644455 mins\n",
      "Train Loss: 0.22739240527153015\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 84.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 4000 iterations: 13.653631444772085 mins\n",
      "Train Loss: 0.202377051115036\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 84.4% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 4200 iterations: 14.33212122519811 mins\n",
      "Train Loss: 0.2577929198741913\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 88.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 88.0, previous best: 86.0\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 4400 iterations: 15.038962817192077 mins\n",
      "Train Loss: 0.1943349838256836\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 88.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 88.0, previous best: 88.0\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 4600 iterations: 15.732861030101777 mins\n",
      "Train Loss: 0.16577455401420593\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 86.4% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 4800 iterations: 16.419993102550507 mins\n",
      "Train Loss: 0.14974185824394226\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 87.6% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 5000 iterations: 17.094246033827464 mins\n",
      "Train Loss: 0.15093247592449188\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 90.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 90.4, previous best: 88.0\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 5200 iterations: 17.78156017859777 mins\n",
      "Train Loss: 0.1367066651582718\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 83.2% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 5400 iterations: 18.481975869337717 mins\n",
      "Train Loss: 0.13279876112937927\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 88.8% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 5600 iterations: 19.17802556355794 mins\n",
      "Train Loss: 0.11047877371311188\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 86.4% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 5800 iterations: 19.859854630629222 mins\n",
      "Train Loss: 0.10888352245092392\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 86.8% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 6000 iterations: 20.542399243513742 mins\n",
      "Train Loss: 0.12456557154655457\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 89.6% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 6200 iterations: 21.22613685131073 mins\n",
      "Train Loss: 0.09966324269771576\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 90.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 6400 iterations: 21.932631353537243 mins\n",
      "Train Loss: 0.1261100172996521\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 82.4% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 6600 iterations: 22.62222111225128 mins\n",
      "Train Loss: 0.08828303962945938\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 92.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 92.0, previous best: 90.4\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 6800 iterations: 23.316270955403645 mins\n",
      "Train Loss: 0.08102218061685562\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 89.2% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 7000 iterations: 23.999796187877656 mins\n",
      "Train Loss: 0.07916553318500519\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 87.2% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 7200 iterations: 24.70307248433431 mins\n",
      "Train Loss: 0.07940500974655151\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 88.4% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 7400 iterations: 25.396244140466056 mins\n",
      "Train Loss: 0.09853415191173553\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 86.8% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 7600 iterations: 26.084302186965942 mins\n",
      "Train Loss: 0.09504225105047226\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 87.6% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 7800 iterations: 26.761401363213857 mins\n",
      "Train Loss: 0.07386739552021027\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 92.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 92.8, previous best: 92.0\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 8000 iterations: 27.432684655984243 mins\n",
      "Train Loss: 0.07369450479745865\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 92.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 8200 iterations: 28.129840286572776 mins\n",
      "Train Loss: 0.1009523943066597\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 90.4% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 8400 iterations: 28.81305510600408 mins\n",
      "Train Loss: 0.06312230974435806\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 92.4% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 8600 iterations: 29.493959613641103 mins\n",
      "Train Loss: 0.10019902884960175\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 90.4% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 8800 iterations: 30.174689348538717 mins\n",
      "Train Loss: 0.06991944462060928\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 92.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 9000 iterations: 30.86705663204193 mins\n",
      "Train Loss: 0.06228838860988617\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 89.6% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 9200 iterations: 31.565758164723714 mins\n",
      "Train Loss: 0.07725644111633301\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 90.8% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 9400 iterations: 32.26214769283931 mins\n",
      "Train Loss: 0.06323187053203583\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 90.4% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 9600 iterations: 32.95124897956848 mins\n",
      "Train Loss: 0.11448059976100922\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 88.8% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 9800 iterations: 33.62265388170878 mins\n",
      "Train Loss: 0.04969604313373566\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 87.6% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 10000 iterations: 34.314950402577715 mins\n",
      "Train Loss: 0.0473782978951931\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 90.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 10200 iterations: 35.0053698182106 mins\n",
      "Train Loss: 0.04834635928273201\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 88.8% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 10400 iterations: 35.68974424203237 mins\n",
      "Train Loss: 0.057063519954681396\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 91.6% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 10600 iterations: 36.368104286988576 mins\n",
      "Train Loss: 0.058810193091630936\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 92.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 92.8, previous best: 92.8\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 10800 iterations: 37.06556243101756 mins\n",
      "Train Loss: 0.04556117579340935\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 91.6% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 11000 iterations: 37.75797124306361 mins\n",
      "Train Loss: 0.04485214129090309\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 93.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 93.6, previous best: 92.8\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 11200 iterations: 38.434472564856215 mins\n",
      "Train Loss: 0.04390806704759598\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 93.2% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 11400 iterations: 39.11860826015472 mins\n",
      "Train Loss: 0.044439949095249176\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 91.2% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 11600 iterations: 39.81000785032908 mins\n",
      "Train Loss: 0.05481942370533943\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 88.4% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 11800 iterations: 40.49147941271464 mins\n",
      "Train Loss: 0.04265614598989487\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 90.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 12000 iterations: 41.16760396162669 mins\n",
      "Train Loss: 0.04193536192178726\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 90.4% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 12200 iterations: 41.841429690519966 mins\n",
      "Train Loss: 0.04052354767918587\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 95.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 95.2, previous best: 93.6\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 12400 iterations: 42.534806597232816 mins\n",
      "Train Loss: 0.04111333191394806\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 90.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 12600 iterations: 43.216274404525755 mins\n",
      "Train Loss: 0.04271691292524338\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 93.2% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 12800 iterations: 43.89813564221064 mins\n",
      "Train Loss: 0.05087221413850784\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 92.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 13000 iterations: 44.569459978739424 mins\n",
      "Train Loss: 0.038603704422712326\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 92.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 13200 iterations: 45.246263003349306 mins\n",
      "Train Loss: 0.05064711347222328\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 88.8% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 13400 iterations: 45.93304834365845 mins\n",
      "Train Loss: 0.03808613866567612\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 92.8% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 13600 iterations: 46.63149930636088 mins\n",
      "Train Loss: 0.06810861825942993\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 90.8% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 13800 iterations: 47.30884855190913 mins\n",
      "Train Loss: 0.036071956157684326\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 89.2% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 14000 iterations: 47.9893749554952 mins\n",
      "Train Loss: 0.05611283332109451\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 92.8% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 14200 iterations: 48.65907975435257 mins\n",
      "Train Loss: 0.03735658526420593\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 91.6% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 14400 iterations: 49.35810221831004 mins\n",
      "Train Loss: 0.036867693066596985\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 91.2% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 14600 iterations: 50.04820246299108 mins\n",
      "Train Loss: 0.034250158816576004\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 90.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 14800 iterations: 50.735707918802895 mins\n",
      "Train Loss: 0.044219911098480225\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 92.8% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 15000 iterations: 51.408844721317294 mins\n",
      "Train Loss: 0.04844006150960922\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 88.8% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 15200 iterations: 52.094417961438495 mins\n",
      "Train Loss: 0.04064428061246872\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 91.6% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 15400 iterations: 52.77628678878148 mins\n",
      "Train Loss: 0.0568125918507576\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 89.6% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 15600 iterations: 53.4553963303566 mins\n",
      "Train Loss: 0.05051635578274727\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 96.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 96.4, previous best: 95.2\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 15800 iterations: 54.116891098022464 mins\n",
      "Train Loss: 0.034407228231430054\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 93.6% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 16000 iterations: 54.81873577038447 mins\n",
      "Train Loss: 0.0817684456706047\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 88.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 16200 iterations: 55.505455370744066 mins\n",
      "Train Loss: 0.0363544225692749\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 93.2% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 16400 iterations: 56.18778986533483 mins\n",
      "Train Loss: 0.03102985955774784\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 92.4% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 16600 iterations: 56.87232658863068 mins\n",
      "Train Loss: 0.03951156884431839\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 92.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 16800 iterations: 57.56872868537903 mins\n",
      "Train Loss: 0.031116629019379616\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 89.6% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 17000 iterations: 58.249188343683876 mins\n",
      "Train Loss: 0.04213552176952362\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 92.8% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 17200 iterations: 58.92953219016393 mins\n",
      "Train Loss: 0.030316898599267006\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 92.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 17400 iterations: 59.612935773531596 mins\n",
      "Train Loss: 0.03684832900762558\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 90.8% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 17600 iterations: 60.32436048587163 mins\n",
      "Train Loss: 0.042668335139751434\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 93.2% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 17800 iterations: 61.012364800771074 mins\n",
      "Train Loss: 0.03664042428135872\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 94.4% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 18000 iterations: 61.701971634229025 mins\n",
      "Train Loss: 0.03137778118252754\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 92.4% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 18200 iterations: 62.38354125022888 mins\n",
      "Train Loss: 0.0317961685359478\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 88.4% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 18400 iterations: 63.05670929352443 mins\n",
      "Train Loss: 0.0294765867292881\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 91.6% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 18600 iterations: 63.746592768033345 mins\n",
      "Train Loss: 0.045950520783662796\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 92.8% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 18800 iterations: 64.43466873168946 mins\n",
      "Train Loss: 0.03236594796180725\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 92.8% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 19000 iterations: 65.1141222079595 mins\n",
      "Train Loss: 0.028350405395030975\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 93.2% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 19200 iterations: 65.79443492889405 mins\n",
      "Train Loss: 0.028512002900242805\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 91.2% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 19400 iterations: 66.49109958012899 mins\n",
      "Train Loss: 0.028803646564483643\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 91.6% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 19600 iterations: 67.17715272108714 mins\n",
      "Train Loss: 0.02845420502126217\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 92.0% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 19800 iterations: 67.85527271429697 mins\n",
      "Train Loss: 0.028059156611561775\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 93.6% 20 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 20000 iterations: 68.53227736552556 mins\n",
      "Train Loss: 0.027349773794412613\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 91.6% 20 way one-shot learning accuracy \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training process!\")\n",
    "print(\"-------------------------------------\")\n",
    "t_start = time.time()\n",
    "for i in range(1, n_iter+1):\n",
    "    (inputs,targets) = createPairs(X,y,sizes,batch_size)\n",
    "    targets=np.asarray(targets)\n",
    "    loss = model.train_on_batch(inputs, targets)\n",
    "    if i % evaluate_every == 0:\n",
    "        print(\"\\n ------------- \\n\")\n",
    "        print(\"Time for {0} iterations: {1} mins\".format(i, (time.time()-t_start)/60.0))\n",
    "        print(\"Train Loss: {0}\".format(loss)) \n",
    "        val_acc = test_oneshot(model,Xval,yval,sizes_val, N_way, n_val, verbose=True)\n",
    "        model.save_weights(os.path.join(model_2_path, 'weights.{}.h5'.format(i)))\n",
    "        if val_acc >= best:\n",
    "            print(\"Current best: {0}, previous best: {1}\".format(val_acc, best))\n",
    "            best = val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcTZ3HHJjbDp"
   },
   "source": [
    "## loading model from weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yt8pqshBxKcQ"
   },
   "outputs": [],
   "source": [
    "# model_path="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "5XSzGDRhjbDr"
   },
   "outputs": [],
   "source": [
    "model.load_weights('weights.20000.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sFJVIYiajbDs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "lDQXUEmFjbDt"
   },
   "outputs": [],
   "source": [
    "def calc_accuracy(N,Xval,yval,anchor_img,anchor_label,model):\n",
    "    count_first=0\n",
    "    count_first3=0\n",
    "    for i in range(N):\n",
    "        ind=random.choice(range(yval.shape[0]))\n",
    "        predicted,anchor_imgs,targets=whichGlyph(model,Xval[ind],anchor_img,anchor_label)\n",
    "        sort_index = np.argsort(np.asarray(predicted).reshape(len(predicted),))\n",
    "        if targets[sort_index[-1]] == yval[ind][0]:\n",
    "            count_first+=1\n",
    "        if yval[ind][0] in targets[sort_index[127:]]:\n",
    "            count_first3+=1\n",
    "    accuracy_first=count_first/N\n",
    "    accuracy_first3=count_first3/N\n",
    "    \n",
    "    return accuracy_first, accuracy_first3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GmLQ6lVYjbDt",
    "outputId": "47886932-fe02-4055-afd5-0524606b7de5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asmaa/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t_start = time.time()\n",
    "acc1,acc3=calc_accuracy(1000,Xval,yval,anchor_img,anchor_label,model)\n",
    "print(f'testing:\\nfound first accuracy = {acc1} , first 3 accuracy = {acc3}')\n",
    "print(\"accuracy fn took {0} mins\".format((time.time()-t_start)/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NdtRB3qAjbDw",
    "outputId": "b66ebb6f-a696-4363-e1dc-ddcc710a49b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training:\n",
      "found first accuracy = 0.972 , first 3 accuracy = 1.0 \n",
      "accuracy fn took 0.447838568687439 mins\n"
     ]
    }
   ],
   "source": [
    "t_start = time.time()\n",
    "acc1,acc3=calc_accuracy(250,X,y,anchor_img,anchor_label,model)\n",
    "print(f'training:\\nfound first accuracy = {acc1} , first 3 accuracy = {acc3} ')\n",
    "print(\"accuracy fn took {0} mins\".format((time.time()-t_start)/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "poAicP_njbDx"
   },
   "outputs": [],
   "source": [
    "# t_start = time.time()\n",
    "# print(calc_accuracy(1,Xval,yval,anchor_img,anchor_label,model))\n",
    "# print(\"accuracy fn took {0} sec\".format((time.time()-t_start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "beW1a29IjbDz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hrLEK0fejbDz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ZplSFXGjbD3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "oneshot_learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
